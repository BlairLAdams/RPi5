#!/usr/bin/env python3
# âœ… Filename: importBronzeKaggleWaterQuality.py

"""
Downloads the 'sahirmaharajj/water-quality-data' dataset from Kaggle,
combines all CSVs, and loads them into the 'bronze' schema of the PostgreSQL
database 'analytics' as a single table named 'seattle_waterquality_raw'.

- Overwrites the table on each run
- Automatically creates the 'bronze' schema if it doesn't exist
- Combines all CSVs into one large DataFrame using outer join logic
- Verifies the table was created successfully
"""

import os
import sys
import kagglehub
import pandas as pd
from sqlalchemy import create_engine, text

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ”§ Database Connection Settings
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

DB_USER = "blair"
DB_PASS = "OneTicket1Rid3"  # ğŸ”’ Consider using getpass for security
DB_NAME = "analytics"
DB_HOST = "localhost"
DB_PORT = "5432"

# ğŸŒ Target geography prefix
GEO_TAG = "seattle"
TABLE_NAME = f"{GEO_TAG}_waterquality_raw"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ“¦ Step 1: Download Dataset from Kaggle
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("ğŸ“¦ Downloading Kaggle dataset 'sahirmaharajj/water-quality-data'...")
try:
    dataset_path = kagglehub.dataset_download("sukhmandeepsinghbrar/water-quality")
except Exception as e:
    print(f"âŒ Kaggle download failed: {e}")
    sys.exit(1)

print(f"âœ… Dataset downloaded at: {dataset_path}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ“‚ Step 2: Locate CSV Files
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

csv_files = [os.path.join(dataset_path, f) for f in os.listdir(dataset_path) if f.endswith('.csv')]
if not csv_files:
    print("ğŸš¨ No CSV files found in downloaded dataset.")
    sys.exit(1)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ”Œ Step 3: Connect to PostgreSQL and Ensure Schema
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

engine_url = f"postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}"
print(f"ğŸ”Œ Connecting to PostgreSQL using engine URL: {engine_url}")
engine = create_engine(engine_url)

try:
    with engine.begin() as conn:
        conn.execute(text("CREATE SCHEMA IF NOT EXISTS bronze;"))
    print("âœ… Schema 'bronze' ensured.")
except Exception as e:
    print(f"âŒ Failed to create schema: {e}")
    sys.exit(1)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ“¥ Step 4: Combine all CSVs and Load Table
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("ğŸ“¥ Reading and combining all CSV files...")

try:
    dataframes = []
    for file in csv_files:
        print(f"  ğŸ”„ Reading {os.path.basename(file)}...")
        df = pd.read_csv(file)
        dataframes.append(df)

    combined_df = pd.concat(dataframes, ignore_index=True, sort=True)
    print(f"âœ… Combined dataset shape: {combined_df.shape}")
    print(f"ğŸ” Columns: {combined_df.columns.tolist()}")

    # Write to PostgreSQL
    print(f"ğŸ“ Writing to bronze.{TABLE_NAME} ...")
    combined_df.to_sql(name=TABLE_NAME, schema="bronze", con=engine, if_exists="replace", index=False)
    print(f"âœ… Write complete: bronze.{TABLE_NAME} ({len(combined_df)} rows).")

    # Verify table creation
    with engine.begin() as conn:
        verify = conn.execute(text(f"""
            SELECT COUNT(*) FROM information_schema.tables 
            WHERE table_schema = 'bronze' AND table_name = '{TABLE_NAME}';
        """)).scalar()

        if verify == 1:
            print(f"âœ… Confirmed: bronze.{TABLE_NAME} exists in 'analytics'.")
        else:
            print(f"âŒ Verification failed: bronze.{TABLE_NAME} not found.")

except Exception as e:
    print(f"âŒ Failed to load combined dataset: {e}")
    sys.exit(1)
