{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Open Source Analytics Platform","text":"<p>This project is a low-cost, modular analytics platform developed on a Raspberry Pi 5 running Raspberry Pi OS Lite in a headless configuration. It is inspired by the spirit of Maker Faire and intended as a learning-oriented, fully open-source experiment in hands-on data engineering. The system design emphasizes simplicity, observability, and reproducibility, while promoting best practices in infrastructure automation and modern data stack tooling.</p>"},{"location":"#purpose-and-philosophy","title":"Purpose and Philosophy","text":"<p>The project demonstrates how a complete analytics stack can be constructed using open-source software and infrastructure-as-code (IaC) principles for approximately $200 in total hardware cost. By combining lightweight services, declarative setup scripts and Python, the platform allows researchers, educators, and technologists to iterate quickly, deploy repeatably, and learn actively in constrained environments.</p> <p>It leverages generative AI (GenAI) tools to accelerate development and emphasizes prompt engineering and quality assurance testing as key capabilities for modern data workflows.</p>"},{"location":"#development-practices","title":"Development Practices","text":"<ul> <li> <p>Infrastructure as Code: All services are installed and configured through modular, version-controlled Bash scripts. This approach supports repeatable system provisioning and reduces manual configuration errors.</p> </li> <li> <p>Open-Source Only: All core components are freely available and widely used in production environments. This ensures transparency, extensibility, and long-term viability.</p> </li> <li> <p>Version Control: Git is used to track all Python code, shell scripts, configuration files, and custom assets. The local repository is restricted to a single directory to streamline backups and maintain clean system boundaries.</p> </li> </ul>"},{"location":"#use-cases","title":"Use Cases","text":"<p>This platform is suited for:</p> <ul> <li>Hands-on learning in modern data engineering</li> <li>Prototyping and testing pipeline orchestration</li> <li>Teaching open-source analytics stack design</li> <li>Exploring GenAI-supported development workflows</li> </ul> <p>Its minimalist hardware footprint and educational emphasis make it ideal for STEM classrooms, bootcamps, independent learners, and civic technologists.</p>"},{"location":"#raspberry-pi-5-specifications","title":"Raspberry Pi 5 Specifications","text":""},{"location":"#overview","title":"Overview","text":"<p>A powerful single-board computer designed for edge computing, prototyping, and embedded analytics \u2014 this version is configured with an upgraded 16GB LPDDR4X RAM module for high-demand workloads such as data orchestration, pipelines, or local inference.</p>"},{"location":"#core-specs","title":"Core Specs","text":"<ul> <li>CPU: Broadcom BCM2712, 4\u00d7 ARM Cortex-A76 @ 2.4GHz</li> <li>GPU: Broadcom VideoCore VII (OpenGL ES 3.1 / Vulkan 1.2)</li> <li>RAM: 16GB LPDDR4X-4267 SDRAM (unofficial/custom board)</li> <li>Storage: 64Gb MicroSD + PCIe 2.0 (via FFC connector), USB 3.0 SSDs</li> <li>Networking:</li> <li>Gigabit Ethernet (real, not USB-attached)</li> <li>Wi-Fi 802.11ac dual-band</li> <li>Bluetooth 5.0</li> </ul>"},{"location":"#io","title":"I/O","text":"<ul> <li>2 \u00d7 USB 3.0</li> <li>2 \u00d7 USB 2.0</li> <li>2 \u00d7 Micro HDMI ports (up to 4Kp60)</li> <li>PCIe Gen 2 single-lane connector (via FFC)</li> <li>GPIO: 40-pin standard header (backward compatible)</li> <li>CSI/DSI camera/display ports (MIPI)</li> </ul>"},{"location":"#power","title":"Power","text":"<ul> <li>USB-C PD (Power Delivery) input</li> <li>Typical draw: ~3A @ 5V under full load</li> <li>Active cooling</li> </ul>"},{"location":"#software","title":"Software","text":"<ul> <li>OS Support:</li> <li>Raspberry Pi OS Lite / Desktop (Bookworm)</li> <li>Ubuntu Server 22.04+</li> <li>Debian ARM64</li> <li>Kernel: Linux 6.1+</li> <li>Bootloader: UEFI support + standard EEPROM configuration</li> <li>Extras:</li> <li>Works with VS Code (Remote SSH)</li> <li>Supports Git, Python, PostgreSQL, MetaData, Grafana, dbt, Dagster, Prometheus, Node_Exporter, MkDocs, and Nginx</li> </ul>"},{"location":"#notes","title":"Notes","text":"<ul> <li>Common among compute-focused lab boards or modified CM5 variants.</li> <li>Suitable for low-footprint AI, data engineering demos, or running full observability stacks.</li> </ul>"},{"location":"AgileAtScale/","title":"Agile at Scale: Program Manager Companion Guide","text":"<p>This guide introduces how to scale Agile principles across multi-team programs, portfolios, or departments\u2014especially in public agencies, utilities, and infrastructure-heavy organizations. It emphasizes adaptability, transparency, and cross-functional coordination, while recognizing governance, budget, and regulatory constraints.</p>"},{"location":"AgileAtScale/#what-is-agile-at-scale","title":"\ud83d\udea6 What Is Agile at Scale?","text":"<p>Agile at Scale applies core Agile principles\u2014iterative delivery, continuous feedback, and adaptive planning\u2014across many teams and stakeholders, including:</p> <ul> <li>Program Management Offices (PMO)</li> <li>Capital Planning</li> <li>IT and OT Teams</li> <li>Regulatory &amp; Compliance Groups</li> <li>Operations &amp; Maintenance (O&amp;M)</li> <li>Field Engineering</li> </ul> <p>Goal: Harmonize delivery across silos by focusing on shared outcomes, transparency, and fast feedback loops.</p>"},{"location":"AgileAtScale/#core-principles","title":"\ud83e\uddf1 Core Principles","text":"Principle Description Customer-Centered Value Prioritize outcomes that deliver measurable value to constituents, users, or regulators. Decentralized Decision-Making Empower teams to own delivery within a shared strategic framework. Incremental Delivery Release usable components regularly\u2014dashboards, forms, integrations, etc.\u2014not just at project close. Transparency Make work, risks, and dependencies visible to all teams and stakeholders. Continuous Feedback Incorporate lessons and stakeholder feedback into each cycle to evolve both solutions and priorities."},{"location":"AgileAtScale/#scaled-agile-planning-layers","title":"\ud83d\uddfa\ufe0f Scaled Agile Planning Layers","text":"<p>Agile at scale is built around aligning multiple planning layers:</p>"},{"location":"AgileAtScale/#1-strategic-themes-annual-multi-year","title":"1. Strategic Themes (Annual / Multi-Year)","text":"<p>Broad agency goals (e.g., water equity, infrastructure modernization, compliance readiness)</p>"},{"location":"AgileAtScale/#2-program-increments-quarterly","title":"2. Program Increments (Quarterly)","text":"<p>Cross-team, outcome-driven goals across multiple initiatives or systems</p>"},{"location":"AgileAtScale/#3-team-sprints-24-weeks","title":"3. Team Sprints (2\u20134 Weeks)","text":"<p>Short delivery cycles focused on working software, integrations, and feedback loops</p>"},{"location":"AgileAtScale/#sample-quarterly-agile-cycle","title":"\ud83d\udd04 Sample Quarterly Agile Cycle","text":"Week Activity 1 Quarterly Planning (define goals, OKRs, team alignments) 2\u201310 Sprints (2\u20134 weeks each) with demos and stakeholder feedback 11 System Integration / User Acceptance Testing (UAT) 12 Retrospective and Replanning for next cycle"},{"location":"AgileAtScale/#agile-roles-in-a-public-sector-or-utility-setting","title":"\ud83d\udee0\ufe0f Agile Roles in a Public Sector or Utility Setting","text":"Role Adaptation Product Owner May be a business analyst, project manager, or subject matter expert focused on stakeholder needs Scrum Master / Delivery Lead Facilitates sprints, removes blockers, ensures Agile principles are followed Program Manager Aligns multiple teams, manages dependencies, integrates strategic planning (OKRs, roadmaps) Team Members May include GIS analysts, SCADA integrators, asset engineers, data modelers, etc."},{"location":"AgileAtScale/#recommended-tools-by-layer","title":"\ud83e\uddf0 Recommended Tools by Layer","text":"Planning Layer Tools Strategic &amp; Program Azure DevOps, Jira Portfolio, MS Planner, Trello Backlog Management Jira, Azure Boards, Trello Feedback &amp; Retrospectives Miro, Mural, Teams Whiteboard Visualization &amp; Reporting Power BI, Confluence, SharePoint"},{"location":"AgileAtScale/#agile-vs-traditional-waterfall-comparison","title":"\ud83e\uddea Agile vs Traditional Waterfall Comparison","text":"Feature Waterfall Agile at Scale Delivery One-time, big bang Continuous, incremental Change Difficult and late-stage Embraced and iterative Planning Fixed upfront Adaptive and ongoing Risk Realized late Identified and mitigated early Feedback Post-launch Every sprint or increment"},{"location":"AgileAtScale/#agile-at-scale-readiness-checklist","title":"\ud83d\udccb Agile at Scale Readiness Checklist","text":"<ul> <li> Do we have cross-functional representation in planning?</li> <li> Are teams empowered to adjust scope in real time?</li> <li> Are stakeholder feedback loops built into every sprint or release?</li> <li> Are workstreams visible to leadership and other teams?</li> <li> Are OKRs or KPIs used to guide priority decisions?</li> </ul>"},{"location":"AgileAtScale/#agile-building-blocks-glossary","title":"\ud83e\udde9 Agile Building Blocks (Glossary)","text":"<ul> <li>Epic: A large body of work that can be broken into smaller stories or tasks</li> <li>Feature: A service or component delivered to users</li> <li>Story: A user-focused unit of work (e.g., \"As a planner, I want to visualize pipe condition...\")</li> <li>Backlog: The prioritized list of upcoming stories or features</li> <li>Sprint: A short, time-boxed development cycle (usually 2\u20134 weeks)</li> <li>Demo / Review: Stakeholder presentation of sprint results</li> <li>Retrospective: Team reflection on process, blockers, and improvements</li> </ul>"},{"location":"AgileAtScale/#pro-tips-for-utilities-government","title":"\ud83d\udccc Pro Tips for Utilities &amp; Government","text":"<ul> <li>Start small: pilot Agile with one cross-functional team before scaling</li> <li>Combine Agile delivery with OKRs to maintain long-term alignment</li> <li>Use Agile boards to expose dependencies and competing priorities</li> <li>Don\u2019t skip retrospectives\u2014they\u2019re vital for cultural change</li> <li>Adapt rituals (e.g., stand-ups, sprints) to field staff or shift schedules</li> </ul>"},{"location":"AgileAtScale/#additional-resources","title":"\ud83d\udcda Additional Resources","text":"<ul> <li>Scaled Agile Framework (SAFe)</li> <li>Agile Government Handbook (18F)</li> <li>Azure DevOps Scaled Agile Practices</li> <li>Scrum Guide</li> </ul>"},{"location":"AgileAtScale/#sample-agile-sprint-template-for-teams","title":"\u2705 Sample Agile Sprint Template (for Teams)","text":"<p>```markdown</p>"},{"location":"AgileAtScale/#sprint-goal","title":"Sprint Goal","text":"<p>Improve water service map accuracy in customer portal</p>"},{"location":"AgileAtScale/#stories-tasks","title":"Stories / Tasks","text":"<ul> <li>Integrate GIS parcel layer into web app backend (GIS Dev)</li> <li>QA cross-street labels with field-verified data (QA/SCADA)</li> <li>Stakeholder review of new zoom-to-address function (UX/PM)</li> </ul>"},{"location":"AgileAtScale/#definition-of-done","title":"Definition of Done","text":"<ul> <li>All tests pass</li> <li>Stakeholder review completed</li> <li>Deployed to staging</li> </ul>"},{"location":"Architectures/","title":"Data Warehouse Architectures","text":"<p>This page introduces foundational and modern data warehouse architectures. It includes the evolution of DW approaches, key pioneers and books, and practical implementation patterns used in enterprise analytics and utility operations.</p>"},{"location":"Architectures/#overview","title":"Overview","text":"<p>A data warehouse (DW) is a centralized platform designed to integrate, store, and organize structured data for analytics, reporting, and business intelligence. It serves as a core component in enterprise data strategy.</p>"},{"location":"Architectures/#objectives","title":"Objectives","text":"<ul> <li>Consolidate cross-system data for consistent analytics</li> <li>Improve report performance and data quality</li> <li>Enable governed, historical, and secure access to enterprise data</li> <li>Support modeling, stewardship, and compliance frameworks</li> </ul>"},{"location":"Architectures/#foundational-frameworks","title":"Foundational Frameworks","text":""},{"location":"Architectures/#bill-inmon-corporate-information-factory","title":"Bill Inmon: Corporate Information Factory","text":"<ul> <li>Known as the father of data warehousing</li> <li>Advocates a top-down approach:</li> <li>Data is modeled in a normalized, 3NF format in the EDW</li> <li>Data marts are created downstream</li> <li>Emphasis on subject-oriented, integrated, non-volatile, and time-variant data</li> </ul> <p>Key Book: Building the Data Warehouse (Inmon, 2005) ISBN: 9780764599446</p>"},{"location":"Architectures/#ralph-kimball-dimensional-modeling","title":"Ralph Kimball: Dimensional Modeling","text":"<ul> <li>Advocates a bottom-up approach:</li> <li>Start with dimensional data marts for immediate value</li> <li>Combine marts into a conformed dimensional warehouse</li> <li>Emphasizes star/snowflake schema, fact/dimension modeling</li> </ul> <p>Key Book: The Data Warehouse Toolkit (Kimball &amp; Ross, 3rd Edition) ISBN: 9781118530801</p>"},{"location":"Architectures/#dan-linstedt-data-vault-architecture","title":"Dan Linstedt: Data Vault Architecture","text":"<ul> <li>Aimed at scalability and auditability in high-change environments</li> <li>Combines elements of both Inmon and Kimball</li> <li>Uses hubs, links, and satellites to track core business concepts and changes</li> </ul> <p>Key Book: Building a Scalable Data Warehouse with Data Vault 2.0 ISBN: 9780128025109</p>"},{"location":"Architectures/#modern-architecture-patterns","title":"Modern Architecture Patterns","text":""},{"location":"Architectures/#1-traditional-layered-architecture","title":"1. Traditional Layered Architecture","text":"<p>Components: - Source Systems \u2192 Staging \u2192 EDW \u2192 Data Marts \u2192 BI Tools - Often built with ETL tools like SSIS, Informatica, Talend</p> <p>Best Fit: - Utilities with batch processing and compliance reporting needs</p>"},{"location":"Architectures/#2-medallion-architecture-bronzesilvergold","title":"2. Medallion Architecture (Bronze\u2013Silver\u2013Gold)","text":"<p>Popularized by: Databricks Stages:</p> <ul> <li>Bronze: Raw ingestion  </li> <li>Silver: Cleaned and filtered  </li> <li>Gold: Aggregated, business-ready</li> </ul> <p>Benefits:</p> <ul> <li>Modular and flexible</li> <li>Supports real-time and batch pipelines</li> <li>Easily integrates with dbt, Dagster, and Lakehouse platforms</li> </ul>"},{"location":"Architectures/#3-lakehouse-architecture","title":"3. Lakehouse Architecture","text":"<p>Combines the openness of data lakes with the reliability of warehouses.</p> <p>Features:</p> <ul> <li>Schema enforcement and ACID (Atomicity, Consistency, Isolation, and Durability) compliance (Delta Lake, Iceberg, Hudi)</li> <li>Supports both structured and semi-structured data</li> <li>Enables analytics and ML from a single platform</li> </ul> <p>Tool Examples:</p> <ul> <li>Databricks, Snowflake, Apache Hudi, Delta Lake, DuckDB</li> </ul>"},{"location":"Architectures/#tool-landscape-by-architecture","title":"Tool Landscape by Architecture","text":"Architecture Tools (Data Platform) Tools (Pipeline &amp; BI) Inmon-style Oracle, SQL Server, PostgreSQL Informatica, SSIS, Cognos Kimball-style Redshift, BigQuery, Snowflake dbt, Power BI, Tableau Data Vault PostgreSQL, Snowflake, Azure SQL VaultSpeed, dbtvault, WhereScape Medallion Delta Lake, DuckDB, BigQuery dbt, Dagster, Great Expectations Lakehouse Databricks, Trino, Iceberg Superset, Metabase, Apache Spark"},{"location":"Architectures/#governance-and-dmbok-alignment","title":"Governance and DMBOK Alignment","text":"<ul> <li>Data Architecture: Defines zones/layers and schema strategy  </li> <li>Data Integration: Staging, orchestration, and lineage  </li> <li>Data Storage &amp; Operations: Performance, reliability, cost  </li> <li>Data Governance: Metadata, ownership, and access policies  </li> <li>Metadata Management: Enables catalogs, glossaries, lineage  </li> </ul>"},{"location":"Architectures/#recommended-reading","title":"Recommended Reading","text":"<ul> <li>The Data Warehouse Toolkit \u2013 Ralph Kimball  </li> <li>Building the Data Warehouse \u2013 Bill Inmon  </li> <li>Data Warehouse Design Solutions \u2013 Kimball et al.  </li> <li>Building a Scalable Data Warehouse with Data Vault 2.0 \u2013 Dan Linstedt  </li> <li>Fundamentals of Data Engineering \u2013 Joe Reis &amp; Matt Housley</li> </ul>"},{"location":"Architectures/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Inmon and Kimball differ in sequencing, but both are foundational</li> <li>Data Vault supports auditability and long-term history</li> <li>Medallion and Lakehouse architectures offer flexibility for modern analytics</li> <li>Choose your approach based on use case, org size, and governance needs</li> </ul>"},{"location":"CMMi/","title":"CMMI Overview for Low-Bureaucracy Teams","text":"<p>A stealthy guide to Capability Maturity Model Integration (CMMI) </p> <p>\"Start small, prove value, scale quietly.\"</p>"},{"location":"CMMi/#executive-summary","title":"\ud83d\ude80 Executive Summary","text":"<p>CMMI (Capability Maturity Model Integration) provides a flexible blueprint for improving business and engineering performance without imposing heavy bureaucracy. Rather than forcing top-down procedures, this guide reframes CMMI as a quiet, culture-aligned toolkit for fast-moving teams that want to scale what already works.</p> <p>This is not about \"compliance.\" It's about reducing avoidable rework, improving team clarity, and making outcomes more repeatable\u2014with as little ceremony as possible.</p> <p>Who it's for: - Startup-style engineering teams - Innovation groups inside enterprises - Consultancies seeking delivery consistency without rigidity - Any org where \u201cprocess\u201d is a loaded word</p> <p>Key Benefits: - Establish repeatable wins without red tape - Build team habits that scale - Gain executive confidence through metrics, not manuals</p>"},{"location":"CMMi/#what-is-cmmi","title":"\ud83e\uddf1 What Is CMMI?","text":"<p>CMMI is a maturity model used to improve delivery consistency, reduce defects, and enable better business outcomes. It's not prescriptive\u2014it's a collection of proven process areas organized into 5 maturity levels.</p> <p>Think of it like a systems-thinking overlay for improving how you work\u2014using the tools and language your team already knows.</p>"},{"location":"CMMi/#why-use-cmmi-in-a-low-bureaucracy-culture","title":"\ud83e\udded Why Use CMMI in a Low-Bureaucracy Culture?","text":"Traditional View Low-Bureaucracy Translation Formal processes Lightweight playbooks, checklists, and rituals Maturity levels Milestone-based goals or OKRs Compliance-driven audits Real-world metrics and peer demos Role definitions Emergent ownership and cross-functional norms <p>Use CMMI to scale craft, not enforce process.</p>"},{"location":"CMMi/#the-5-maturity-levels-reimagined","title":"\ud83c\udfd7\ufe0f The 5 Maturity Levels (Reimagined)","text":"<ol> <li>Initial (Ad Hoc but Heroic) </li> <li>What: Success depends on individual effort.  </li> <li> <p>Stealth move: Identify accidental brilliance worth repeating.</p> </li> <li> <p>Managed (Repeatable Success) </p> </li> <li>What: Teams start building routines.  </li> <li> <p>Stealth move: Add minimal templates, \u201cjust enough\u201d documentation, and shared rituals.</p> </li> <li> <p>Defined (Team Muscle Memory) </p> </li> <li>What: Processes are intentional, taught, and improvable.  </li> <li> <p>Stealth move: Capture what\u2019s working and formalize only what\u2019s fragile.</p> </li> <li> <p>Quantitatively Managed (Proof Beats Opinion) </p> </li> <li>What: Decisions are driven by simple, shared metrics.  </li> <li> <p>Stealth move: Dashboards, lightweight KPIs, and delivery baselines.</p> </li> <li> <p>Optimizing (We Get Better on Purpose) </p> </li> <li>What: The system improves itself through experimentation.  </li> <li>Stealth move: Innovation sprints, retrospective-driven change, postmortem culture.</li> </ol>"},{"location":"CMMi/#key-cmmi-process-areas-translated-for-agile","title":"\u2699\ufe0f Key CMMI Process Areas (Translated for Agile)","text":"CMMI Area Low-Bureaucracy Equivalent Requirements Management Lean user stories, stakeholder OKRs Project Planning Kanban boards, sprint planning, roadmap reviews Process Monitoring Lightweight retros, burndown dashboards Configuration Management Git, version tags, changelogs Quality Assurance Peer review, linter gates, \u201cDefinition of Done\u201d Risk Management Pre-mortems, Slack call-outs, parking lot docs"},{"location":"CMMi/#implementation-strategy-quiet-wins","title":"\ud83d\udd12 Implementation Strategy: Quiet Wins","text":""},{"location":"CMMi/#1-use-what-you-already-do","title":"1. Use What You Already Do","text":"<p>Translate current workflows into maturity-building practices. Example: GitHub commits = config management.</p>"},{"location":"CMMi/#2-integrate-into-existing-rituals","title":"2. Integrate Into Existing Rituals","text":"<p>Add one question to your retros: \u201cWhat went surprisingly well?\u201d Capture and replay that.</p>"},{"location":"CMMi/#3-show-outcomes-not-artifacts","title":"3. Show Outcomes, Not Artifacts","text":"<p>Instead of a binder, show a dashboard or a velocity trend to leadership.</p>"},{"location":"CMMi/#4-build-as-you-go","title":"4. Build as You Go","text":"<p>Formalize only what\u2019s fragile or repeating. Don\u2019t invent process\u2014evolve it.</p>"},{"location":"CMMi/#okr-aligned-implementation-tasks","title":"\ud83c\udfaf OKR-Aligned Implementation Tasks","text":""},{"location":"CMMi/#objective-1-reduce-delivery-volatility-across-teams","title":"Objective 1: Reduce delivery volatility across teams","text":"<ul> <li>KR1.1: Implement weekly or bi-weekly retrospectives in all teams  </li> <li>KR1.2: Document at least 3 high-variance workflows using lightweight templates  </li> <li>KR1.3: Define and track \u201clead time to deploy\u201d baseline</li> </ul>"},{"location":"CMMi/#objective-2-increase-consistency-in-scope-and-outcome-estimation","title":"Objective 2: Increase consistency in scope and outcome estimation","text":"<ul> <li>KR2.1: Pilot shared estimation guidelines (e.g., t-shirt sizing + velocity targets)  </li> <li>KR2.2: Track estimation accuracy for 3 recent projects  </li> <li>KR2.3: Introduce \u201cscope creep log\u201d as part of sprint review agenda</li> </ul>"},{"location":"CMMi/#objective-3-build-shared-improvement-habits-without-bureaucracy","title":"Objective 3: Build shared improvement habits without bureaucracy","text":"<ul> <li>KR3.1: Enable team-defined checklists for readiness or completion  </li> <li>KR3.2: Launch 2 improvement experiments with a \u201cwhat-if\u201d success metric  </li> <li>KR3.3: Celebrate 1 optimization per quarter in an all-hands/demo day</li> </ul>"},{"location":"CMMi/#objective-4-establish-lightweight-traceability-without-slowing-delivery","title":"Objective 4: Establish lightweight traceability without slowing delivery","text":"<ul> <li>KR4.1: Auto-generate changelogs from GitHub PRs or commits  </li> <li>KR4.2: Link user stories to business OKRs or customer value statements  </li> <li>KR4.3: Use labels/tags to track project origin, type, and outcome</li> </ul>"},{"location":"CMMi/#final-thought","title":"\ud83d\udcac Final Thought","text":"<p>\u201cIf it works, ship it. If it breaks, improve it. That\u2019s CMMI done right.\u201d</p> <p>This guide isn\u2019t about raising overhead. It\u2019s about scaling competence, predictability, and trust, using habits your teams already value.</p>"},{"location":"CognitiveBiases/","title":"Cognitive Biases in Leadership and Decision-Making","text":"<p>This guide summarizes key cognitive biases most relevant to leadership, strategic decision-making, and organizational behavior. Each includes practical mitigation strategies to support awareness and better decision hygiene.</p>"},{"location":"CognitiveBiases/#1-information-evaluation-biases","title":"1. Information Evaluation Biases","text":""},{"location":"CognitiveBiases/#confirmation-bias","title":"Confirmation Bias","text":"<p>Favoring information that supports existing beliefs while discounting contradictions.</p> <ul> <li>Encourage pre-mortems or red-team reviews.</li> <li>Assign team members to play devil\u2019s advocate.</li> <li>Actively seek out disconfirming data.</li> </ul>"},{"location":"CognitiveBiases/#anchoring-bias","title":"Anchoring Bias","text":"<p>Overweighing the first number or idea presented.</p> <ul> <li>Generate multiple independent estimates.</li> <li>Delay discussion of first proposals in group settings.</li> <li>Use structured decision templates.</li> </ul>"},{"location":"CognitiveBiases/#availability-heuristic","title":"Availability Heuristic","text":"<p>Relying on recent or vivid events rather than probabilities.</p> <ul> <li>Use historical or statistical baselines.</li> <li>Track actual frequency of events.</li> <li>Document rationale behind decisions.</li> </ul>"},{"location":"CognitiveBiases/#framing-effect","title":"Framing Effect","text":"<p>Decisions change based on presentation (gain vs. loss).</p> <ul> <li>Reframe problems in multiple ways.</li> <li>Present both positive and negative framings side-by-side.</li> <li>Use data visualizations to balance framing.</li> </ul>"},{"location":"CognitiveBiases/#base-rate-neglect","title":"Base Rate Neglect","text":"<p>Ignoring general statistics in favor of specific anecdotes.</p> <ul> <li>Start with population-level probabilities.</li> <li>Anchor decisions in baseline metrics before adjusting.</li> <li>Train teams to distinguish anecdote from trend.</li> </ul>"},{"location":"CognitiveBiases/#2-overconfidence-and-ego-biases","title":"2. Overconfidence and Ego Biases","text":""},{"location":"CognitiveBiases/#overconfidence-effect","title":"Overconfidence Effect","text":"<p>Overestimating accuracy or control over outcomes.</p> <ul> <li>Use calibration exercises with forecast ranges.</li> <li>Review historical forecasting errors.</li> <li>Build in contingency and scenario planning.</li> </ul>"},{"location":"CognitiveBiases/#dunning-kruger-effect","title":"Dunning-Kruger Effect","text":"<p>Incompetent people overrate themselves; experts underrate themselves.</p> <ul> <li>Promote peer reviews and 360 feedback.</li> <li>Establish mentorship loops across experience levels.</li> <li>Encourage humility through after-action reviews.</li> </ul>"},{"location":"CognitiveBiases/#illusion-of-control","title":"Illusion of Control","text":"<p>Overstating influence over external events.</p> <ul> <li>Map out controllable vs. uncontrollable factors.</li> <li>Rehearse worst-case scenarios without self-blame.</li> <li>Use probabilistic modeling for strategic planning.</li> </ul>"},{"location":"CognitiveBiases/#optimism-bias","title":"Optimism Bias","text":"<p>Expecting outcomes to turn out better than realistic.</p> <ul> <li>Perform \u201cprospective hindsight\u201d or failure scenario planning.</li> <li>Use rolling forecasts instead of fixed projections.</li> <li>Involve external reviewers to stress-test assumptions.</li> </ul>"},{"location":"CognitiveBiases/#3-social-and-organizational-biases","title":"3. Social and Organizational Biases","text":""},{"location":"CognitiveBiases/#groupthink","title":"Groupthink","text":"<p>Suppressing dissent to preserve harmony.</p> <ul> <li>Use anonymous input or surveys before group discussion.</li> <li>Rotate meeting facilitators to avoid dominance.</li> <li>Normalize constructive disagreement.</li> </ul>"},{"location":"CognitiveBiases/#in-group-bias","title":"In-group Bias","text":"<p>Favoring familiar or similar individuals.</p> <ul> <li>Create cross-functional project teams.</li> <li>Blind key decision steps (e.g. hiring, promotions).</li> <li>Foster psychological safety for outside perspectives.</li> </ul>"},{"location":"CognitiveBiases/#authority-bias","title":"Authority Bias","text":"<p>Overweighting opinions from leaders or experts.</p> <ul> <li>Evaluate claims independent of source prestige.</li> <li>Facilitate bottom-up feedback mechanisms.</li> <li>Ask: \u201cWhat evidence supports this recommendation?\u201d</li> </ul>"},{"location":"CognitiveBiases/#false-consensus-effect","title":"False Consensus Effect","text":"<p>Believing others share your views more than they do.</p> <ul> <li>Run stakeholder mapping exercises.</li> <li>Validate assumptions through structured interviews or surveys.</li> <li>Promote \u201cspeak-up\u201d culture explicitly.</li> </ul>"},{"location":"CognitiveBiases/#stereotyping","title":"Stereotyping","text":"<p>Applying group assumptions to individuals.</p> <ul> <li>Focus evaluations on observed behavior, not labels.</li> <li>Use competency-based frameworks.</li> <li>Invest in bias-awareness training.</li> </ul>"},{"location":"CognitiveBiases/#4-risk-and-uncertainty-biases","title":"4. Risk and Uncertainty Biases","text":""},{"location":"CognitiveBiases/#loss-aversion","title":"Loss Aversion","text":"<p>Avoiding losses even when gains are rational.</p> <ul> <li>Frame risks as opportunity costs.</li> <li>Compare trade-offs explicitly.</li> <li>Run simulations of potential upside vs. downside.</li> </ul>"},{"location":"CognitiveBiases/#status-quo-bias","title":"Status Quo Bias","text":"<p>Preferring the current state even if change is beneficial.</p> <ul> <li>Challenge default options during reviews.</li> <li>Run cost-of-inaction scenarios.</li> <li>Use pilots or experiments to lower perceived risk of change.</li> </ul>"},{"location":"CognitiveBiases/#sunk-cost-fallacy","title":"Sunk Cost Fallacy","text":"<p>Continuing a failing course due to past investment.</p> <ul> <li>Ask: \u201cWould we invest in this today if we hadn\u2019t already?\u201d</li> <li>Hold regular portfolio or project review checkpoints.</li> <li>Empower teams to pivot without penalty.</li> </ul>"},{"location":"CognitiveBiases/#planning-fallacy","title":"Planning Fallacy","text":"<p>Underestimating effort or time needed.</p> <ul> <li>Use past project data for effort estimation.</li> <li>Include buffer time explicitly in planning.</li> <li>Conduct cross-team estimation reviews.</li> </ul>"},{"location":"CognitiveBiases/#neglect-of-probability","title":"Neglect of Probability","text":"<p>Misjudging or ignoring likelihoods of events.</p> <ul> <li>Use risk matrices and likelihood-impact scoring.</li> <li>Communicate in absolute frequencies (\u201c1 in 20\u201d) not vague terms (\u201cunlikely\u201d).</li> <li>Emphasize tail-risk planning in critical decisions.</li> </ul>"},{"location":"CognitiveBiases/#how-to-use-this-guide","title":"How to Use This Guide","text":"<p>These biases are not flaws\u2014they\u2019re mental shortcuts designed for speed over accuracy. But in leadership, where stakes are high and systems are complex, surfacing and managing them is essential.</p> <p>Use this guide to:</p> <ul> <li>Facilitate leadership workshops and retrospectives  </li> <li>Guide team decision reviews  </li> <li>Structure project postmortems  </li> <li>Support executive coaching  </li> <li>Develop training on critical thinking</li> </ul> <p>Bias-aware leaders make clearer, more inclusive, and more resilient decisions.</p>"},{"location":"DesignThinking/","title":"Design Thinking Overview","text":"<p>This page introduces Design Thinking \u2014 a human-centered, iterative approach to solving complex problems. It is widely used in product design, service innovation, digital transformation, and public sector modernization.</p>"},{"location":"DesignThinking/#definition","title":"Definition","text":"<p>Design Thinking is a structured methodology for creative problem solving. It emphasizes empathy with users, rapid prototyping, and iterative testing \u2014 enabling teams to create solutions that are desirable, feasible, and viable.</p>"},{"location":"DesignThinking/#purpose","title":"Purpose","text":"<ul> <li>Center problem-solving on real user needs  </li> <li>Reduce risk of delivering the wrong solution  </li> <li>Encourage experimentation and rapid iteration  </li> <li>Bridge silos through cross-functional collaboration  </li> <li>Accelerate value delivery in uncertain or evolving contexts</li> </ul>"},{"location":"DesignThinking/#five-stages-of-design-thinking","title":"Five Stages of Design Thinking","text":"<p>Design Thinking is typically described in five stages. While the process is not strictly linear, teams often cycle between stages as needed.</p>"},{"location":"DesignThinking/#1-empathize","title":"1. Empathize","text":"<p>Understand users and their experiences through observation, interviews, and engagement.</p>"},{"location":"DesignThinking/#2-define","title":"2. Define","text":"<p>Synthesize findings into a clear problem statement (point of view) that captures user needs.</p>"},{"location":"DesignThinking/#3-ideate","title":"3. Ideate","text":"<p>Generate a wide range of potential solutions without judgment or bias.</p>"},{"location":"DesignThinking/#4-prototype","title":"4. Prototype","text":"<p>Build simple, testable versions of concepts to explore and validate ideas.</p>"},{"location":"DesignThinking/#5-test","title":"5. Test","text":"<p>Try out prototypes with users, gather feedback, and refine the solution.</p>"},{"location":"DesignThinking/#common-terminology","title":"Common Terminology","text":"Term Definition Empathy Map A tool for visualizing user attitudes and behaviors Point of View (POV) A focused problem definition grounded in user needs HMW (How Might We) A brainstorming prompt that opens space for multiple solutions Prototype A simplified version of a solution used for testing and feedback Iteration A loop of continuous learning, feedback, and refinement"},{"location":"DesignThinking/#use-cases-in-utilities-and-public-agencies","title":"Use Cases in Utilities and Public Agencies","text":"<ul> <li>Reimagining customer billing or outreach processes  </li> <li>Prototyping field worker mobile tools or dashboards  </li> <li>Improving internal workflows (e.g., service requests, asset inspections)  </li> <li>Engaging stakeholders in participatory problem-solving  </li> <li>Aligning digital services with equity, accessibility, and resilience goals</li> </ul>"},{"location":"DesignThinking/#design-thinking-vs-traditional-problem-solving","title":"Design Thinking vs Traditional Problem Solving","text":"Feature Design Thinking Traditional Planning Focus User experience and insight Technical requirements and feasibility Process Style Iterative and non-linear Linear and milestone-driven Risk Profile Early failure and learning Late risk discovery Collaboration Cross-functional, inclusive Role-based or hierarchical Speed Rapid prototyping Extended planning and review"},{"location":"DesignThinking/#benefits","title":"Benefits","text":"<ul> <li>Reduces time spent solving the wrong problem  </li> <li>Encourages shared understanding among diverse teams  </li> <li>Builds trust and engagement through co-creation  </li> <li>Promotes evidence-based design choices  </li> <li>Supports iterative delivery and agile practices</li> </ul>"},{"location":"DesignThinking/#references-and-further-reading","title":"References and Further Reading","text":"<ul> <li> <p>Change by Design \u2013 Tim Brown (IDEO)   ISBN: 9780061766084</p> </li> <li> <p>The Field Guide to Human-Centered Design \u2013 IDEO.org   Free download: https://www.designkit.org/resources/1</p> </li> <li> <p>Design Thinking: Understand \u2013 Improve \u2013 Apply \u2013 Plattner, Meinel, Leifer (HPI)   ISBN: 9783319196424</p> </li> <li> <p>Stanford d.school resources   https://dschool.stanford.edu/resources</p> </li> <li> <p>IBM Design Thinking Framework   https://www.ibm.com/design/thinking/</p> </li> <li> <p>IDEO Design Kit   https://www.designkit.org/</p> </li> </ul>"},{"location":"EDWHarmonization/","title":"EDW, BI, ML, &amp; AI Harmonization","text":"<p>A practical guide to aligning analytics, machine learning, and data platforms</p>"},{"location":"EDWHarmonization/#executive-summary","title":"\ud83d\ude80 Executive Summary","text":"<p>Artificial Intelligence (AI) and Machine Learning (ML) thrive when powered by clean, reliable, and well-understood data. An Enterprise Data Warehouse (EDW) and Business Intelligence (BI) system provide the foundational infrastructure, governance, and visibility that make AI/ML not only possible\u2014but scalable and trustworthy.</p> <p>This guide explains how EDW and BI tools complement AI/ML\u2014and when simpler tools like rules, dashboards, or statistical methods can be more appropriate and effective.</p>"},{"location":"EDWHarmonization/#key-concepts","title":"\ud83d\udd0d Key Concepts","text":"<ul> <li>AI/ML: Algorithms that learn from data to make predictions, detect patterns, or automate decisions.</li> <li>EDW (Enterprise Data Warehouse): A centralized data hub integrating and cleansing data from across the enterprise.</li> <li>BI (Business Intelligence): Tools and dashboards that provide structured reporting, visualization, and alerts.</li> </ul>"},{"location":"EDWHarmonization/#how-they-complement-each-other","title":"\ud83e\udde0 How They Complement Each Other","text":""},{"location":"EDWHarmonization/#1-edw-enables-data-quality-and-availability-for-ml","title":"1. EDW Enables Data Quality and Availability for ML","text":"<ul> <li>How: Centralizes raw and cleaned data from multiple sources.  </li> <li>Why: ML models require clean, structured, consistent data to be effective.</li> </ul>"},{"location":"EDWHarmonization/#2-bi-provides-explainability-and-feedback-loops","title":"2. BI Provides Explainability and Feedback Loops","text":"<ul> <li>How: Visualizes model outputs and supports drill-down for errors or outliers.  </li> <li>Why: Builds trust with stakeholders and supports continuous improvement.</li> </ul>"},{"location":"EDWHarmonization/#3-edw-supports-feature-engineering-at-scale","title":"3. EDW Supports Feature Engineering at Scale","text":"<ul> <li>How: Provides historical, joined, and normalized data ready for transformation.  </li> <li>Why: Enables repeatable, traceable features across models and time.</li> </ul>"},{"location":"EDWHarmonization/#4-bi-drives-use-cases-and-adoption","title":"4. BI Drives Use Cases and Adoption","text":"<ul> <li>How: Identifies patterns, anomalies, and outliers that prompt model development.  </li> <li>Why: Helps define real-world business problems worth modeling.</li> </ul>"},{"location":"EDWHarmonization/#5-edw-and-bi-provide-governance-security-and-trust","title":"5. EDW and BI Provide Governance, Security, and Trust","text":"<ul> <li>How: Role-based access control, audit logging, and metric definitions.  </li> <li>Why: Enables responsible AI/ML use and regulatory compliance.</li> </ul>"},{"location":"EDWHarmonization/#when-aiml-may-be-overkill","title":"\u2696\ufe0f When AI/ML May Be Overkill","text":""},{"location":"EDWHarmonization/#use-cases-better-served-by-bi-rules-or-stats","title":"Use Cases Better Served by BI, Rules, or Stats","text":"<ol> <li>KPI Monitoring </li> <li> <p>Prefer: Dashboards with thresholds and alerts</p> </li> <li> <p>Root Cause Analysis </p> </li> <li> <p>Prefer: SQL segmentations, control charts</p> </li> <li> <p>Simple Forecasting </p> </li> <li> <p>Prefer: Linear regression or ARIMA</p> </li> <li> <p>Anomaly Detection in Small Data </p> </li> <li> <p>Prefer: Z-scores, control limits</p> </li> <li> <p>Deterministic Decision Logic </p> </li> <li> <p>Prefer: Rules engines, validation frameworks</p> </li> <li> <p>Compliance Monitoring </p> </li> <li>Prefer: BI + alerts with documented exception thresholds</li> </ol>"},{"location":"EDWHarmonization/#ro-membrane-maintenance-a-real-world-bi-vs-aiml-example","title":"\ud83d\udca7 RO Membrane Maintenance \u2013 A Real-World BI vs. AI/ML Example","text":""},{"location":"EDWHarmonization/#scenario","title":"Scenario:","text":"<p>A utility tracks Reverse Osmosis (RO) system performance using: - Permeate flow rate (GPM) - Salinity or conductivity (\u00b5S/cm) - Pump energy usage (kWh) - Membrane pressure drop (psi)</p>"},{"location":"EDWHarmonization/#option-a-bi-real-time-alerting","title":"Option A: BI + Real-Time Alerting","text":"<ul> <li>Rule: \u201cIf flow rate drops &gt;10% over 3 days AND conductivity rises &gt;15% \u2192 flag\u201d  </li> <li>Dashboards show trends and alert thresholds  </li> <li>Simple and explainable</li> </ul>"},{"location":"EDWHarmonization/#option-b-aiml-anomaly-detection","title":"Option B: AI/ML Anomaly Detection","text":"<ul> <li>Complex modeling of multivariate signals  </li> <li>May require retraining and tuning  </li> <li>Risk of false positives or misinterpretation</li> </ul> <p>Verdict: BI is faster, cheaper, and more transparent. ML adds value only if predictive accuracy significantly improves ROI.</p>"},{"location":"EDWHarmonization/#ai-readiness-checklist","title":"\ud83e\uddea AI Readiness Checklist","text":"Question If \"No\", Consider BI or Rules Do we have 6\u201312 months of clean, historical data? \u2705 Can we clearly define a prediction target (label)? \u2705 Are patterns nonlinear, complex, or multidimensional? \u2705 Would the outcome be hard to solve with rules or stats? \u2705 Is someone available to monitor and retrain the model? \u2705 Do stakeholders need explainability for decisions? \u274c (use BI if yes) Is the business impact worth the development effort? \u2705"},{"location":"EDWHarmonization/#just-enough-ai-strategy","title":"\u2696\ufe0f \u201cJust Enough AI\u201d Strategy","text":"<p>What is it? \u201cJust Enough AI\u201d is a practical approach that avoids AI theater or overengineering by using lightweight ML where it adds value\u2014and defers to BI, statistics, or rule-based logic when those are faster and clearer.</p>"},{"location":"EDWHarmonization/#use-techniques-like","title":"Use Techniques Like:","text":"<ul> <li>Regression and classification with explainable models (e.g., logistic regression, decision trees) </li> <li>AutoML tools to test feasibility quickly without full engineering  </li> <li>Hybrid pipelines: rules first, model fallback (e.g., escalate only ambiguous cases to ML)  </li> <li>Batch ML with BI visualization: models run nightly, results shown in Metabase or Power BI  </li> </ul>"},{"location":"EDWHarmonization/#when-to-use","title":"When to Use:","text":"<ul> <li>When rules don\u2019t work every time, but cover most cases  </li> <li>When there\u2019s measurable but not mission-critical benefit  </li> <li>When you need just enough prediction to aid human decision-making  </li> <li>When you're building toward more advanced ML, but need results now  </li> </ul>"},{"location":"EDWHarmonization/#example","title":"Example:","text":"<ul> <li>Start with BI rules for RO maintenance (thresholds)  </li> <li>Later introduce ML to rank severity or suggest optimal CIP timing based on multiple variables</li> </ul>"},{"location":"EDWHarmonization/#summary-matrix","title":"\u2705 Summary Matrix","text":"Role EDW Contribution BI Contribution Data Quality Cleansing, lineage, conformed dims Real-time validations, anomaly detection Feature Engineering Joins, windowing, historic snapshots Feedback loops, visual tuning Model Performance Data freshness, consistency Dashboards for monitoring, alerts Business Adoption Trusted source of truth Explainable results, embedded insights Governance &amp; Compliance Role-based access, data contracts Usage tracking, audit trails Low-Complexity Use Cases Aggregation, rules, trends Faster results with BI or statistics Just Enough AI Simplified modeling and fallback BI-enhanced ML transparency"},{"location":"EDWHarmonization/#final-thought","title":"\ud83e\udded Final Thought","text":"<p>\"The smartest AI strategy is the one that solves the problem \u2014 not the one that sounds smartest.\"</p> <p>BI and EDW are your launchpad for AI, but often, they\u2019re all you need. Use \u201cJust Enough AI\u201d to solve real problems with clarity, speed, and trust\u2014and scale up only when complexity demands it.</p>"},{"location":"GovernanceFrameworks/","title":"Data Governance Frameworks","text":"<p>This page outlines key data governance frameworks relevant to utilities, public agencies, and enterprises undergoing analytics modernization, compliance alignment, or digital transformation. It compares models and supports framework selection based on strategic priorities and organizational context.</p>"},{"location":"GovernanceFrameworks/#definition","title":"Definition","text":"<p>Data governance is the discipline of defining, implementing, and enforcing the policies, processes, roles, and standards that ensure high-quality, secure, and trustworthy data across an organization.</p>"},{"location":"GovernanceFrameworks/#objectives","title":"Objectives","text":"<ul> <li>Establish clear accountability for data assets  </li> <li>Ensure compliance with regulatory and security requirements  </li> <li>Enable reliable analytics and reporting  </li> <li>Support enterprise-wide data quality and stewardship  </li> <li>Align data-related decisions with strategic and operational goals  </li> </ul>"},{"location":"GovernanceFrameworks/#core-components-of-a-governance-framework","title":"Core Components of a Governance Framework","text":"<ol> <li> <p>Policies and Standards    Naming conventions, data classifications, retention, access</p> </li> <li> <p>Processes and Workflows    Issue resolution, access approval, data lifecycle management</p> </li> <li> <p>Roles and Responsibilities    Owners, stewards, custodians, governance councils</p> </li> <li> <p>Metadata and Lineage    Catalogs, business glossaries, lineage visualization</p> </li> <li> <p>Monitoring and Metrics    Quality scoring, policy compliance, SLA adherence</p> </li> </ol>"},{"location":"GovernanceFrameworks/#frameworks-commonly-used-in-utilities","title":"Frameworks Commonly Used in Utilities","text":""},{"location":"GovernanceFrameworks/#1-dmbok-data-management-body-of-knowledge","title":"1. DMBOK (Data Management Body of Knowledge)","text":"<ul> <li>Published by DAMA International  </li> <li>Defines 11 data management knowledge areas  </li> <li>Widely used in utilities, public sector, and regulated industries  </li> <li>Emphasizes governance maturity and stewardship</li> </ul>"},{"location":"GovernanceFrameworks/#2-awwa-g430-g480-standards","title":"2. AWWA G430 / G480 Standards","text":"<ul> <li>Performance and asset management standards specific to utilities  </li> <li>G430 focuses on security and operational risk  </li> <li>G480 provides a utility management system model, including reporting, planning, and data use  </li> <li>Frequently used in performance benchmarking and audits</li> </ul>"},{"location":"GovernanceFrameworks/#3-iso-8000","title":"3. ISO 8000","text":"<ul> <li>International standard for data quality and master data exchange  </li> <li>Emphasizes data portability, provenance, and validation  </li> <li>Aligns well with SCADA, GIS, and public infrastructure data systems</li> </ul>"},{"location":"GovernanceFrameworks/#4-dcam-data-management-capability-assessment-model","title":"4. DCAM (Data Management Capability Assessment Model)","text":"<ul> <li>Developed by the EDM Council  </li> <li>Focuses on enterprise data governance maturity  </li> <li>Domains include data ownership, quality, lineage, and platform enablement  </li> <li>Applied in utilities with digital governance and data product strategies</li> </ul>"},{"location":"GovernanceFrameworks/#5-nist-rmf-and-csf","title":"5. NIST RMF and CSF","text":"<ul> <li>Frameworks for cybersecurity, asset inventory, and resilience  </li> <li>Includes risk-based categorization, protection, and recovery planning  </li> <li>Common in SCADA/ICS environments and federally funded infrastructure</li> </ul>"},{"location":"GovernanceFrameworks/#6-cobit","title":"6. COBIT","text":"<ul> <li>IT governance and internal control framework  </li> <li>Supports accountability, process auditability, and maturity modeling  </li> <li>Often used in enterprise IT and CIO-led governance programs</li> </ul>"},{"location":"GovernanceFrameworks/#7-coso-erm-enterprise-risk-management","title":"7. COSO ERM (Enterprise Risk Management)","text":"<ul> <li>Control and compliance-oriented framework used in finance and utility board governance  </li> <li>Aligns with internal audit functions, SOX compliance, and organizational risk appetite  </li> <li>Complements NIST and COBIT in larger or more regulated environments</li> </ul>"},{"location":"GovernanceFrameworks/#8-custom-or-hybrid-approaches","title":"8. Custom or Hybrid Approaches","text":"<ul> <li>Many utilities combine AWWA, DMBOK, or DCAM with internal charters and regulatory policies  </li> <li>Governance-as-practice approaches include OKR dashboards, data councils, and agile stewardship  </li> <li>Hybrids reflect utility-specific regulatory context, staffing realities, and legacy systems</li> </ul>"},{"location":"GovernanceFrameworks/#framework-comparison","title":"Framework Comparison","text":"Framework Sector Fit Orientation Common Use Cases DMBOK General Lifecycle-based Public agencies, regulated infrastructure AWWA G430/G480 Water utilities Operational, standards-based Accreditation, board review, audits ISO 8000 Industrial/global Data quality &amp; exchange SCADA, GIS, sensor telemetry DCAM Enterprise/finance Capability maturity Governance maturity, platform enablement NIST RMF/CSF Critical infra Cybersecurity &amp; resilience SCADA/ICS protection, federal mandates COBIT IT/Finance Audit/process CIO oversight, internal controls COSO ERM Board/Audit Risk and compliance SOX, finance, executive governance Hybrid Cross-functional Contextual, agile OKR dashboards, stewardship councils"},{"location":"GovernanceFrameworks/#governance-roles","title":"Governance Roles","text":"<ul> <li>Data Owner \u2013 Accountable for data quality and authorized use  </li> <li>Data Steward \u2013 Maintains definitions, rules, and term consistency  </li> <li>Data Custodian \u2013 Manages infrastructure, access control, and backups  </li> <li>Governance Council \u2013 Approves standards, reviews escalations, ensures alignment  </li> <li>Data Consumer \u2013 Uses governed data for operations, analytics, or decisions  </li> </ul>"},{"location":"GovernanceFrameworks/#governance-metrics","title":"Governance Metrics","text":"<ul> <li>Percentage of systems with assigned data owners  </li> <li>Share of critical data elements documented in glossary/catalog  </li> <li>Ratio of Tier 1 datasets with defined quality rules  </li> <li>Average time to resolve steward-raised issues  </li> <li>Adoption rate of standardized business definitions in reporting</li> </ul>"},{"location":"GovernanceFrameworks/#okrs-for-governance-maturity","title":"OKRs for Governance Maturity","text":"<p>Objective: Establish a trusted, transparent data governance foundation that supports operational reliability, regulatory compliance, and enterprise analytics.</p> <p>Key Results: - Assign ownership to 100% of Tier 1 datasets by Q2 - Document business glossary terms for all finance and asset systems by Q3 - Reduce unresolved data quality issues (&gt;30 days) by 90% - Launch automated metadata workflows with steward feedback by year-end  </p>"},{"location":"GovernanceFrameworks/#further-reading-pioneers-and-influencers","title":"Further Reading: Pioneers and Influencers","text":"<p>John Ladley - Author of Data Governance: How to Design, Deploy, and Sustain an Effective Program - https://johnladley.com</p> <p>Gwen Thomas - Founder of the Data Governance Institute (DGI) - Developed the DGI Framework - https://www.datagovernance.com/</p> <p>DAMA International - Publisher of the DAMA-DMBOK standard - https://www.dama.org</p> <p>Anne Marie Smith - DMBOK contributor and advisor on governance maturity models</p> <p>Tony Fisher - Author of The Data Asset: How Smart Companies Govern Their Data </p> <p>Sunil Soares - Author of The IBM Data Governance Unified Process - https://www.sunilsoares.com/</p> <p>EDM Council - Publisher of DCAM, widely used in regulated sectors - https://edmcouncil.org/</p> <p>NIST (National Institute of Standards and Technology) - Publisher of CSF and RMF frameworks for critical infrastructure - https://www.nist.gov</p>"},{"location":"Kanban/","title":"Agile Kanban Overview","text":"<p>This page introduces Agile Kanban \u2014 a visual workflow management method used to organize, prioritize, and deliver work efficiently. Kanban is widely applied in data teams, IT operations, DevOps, and governance programs where flexibility and flow are critical.</p>"},{"location":"Kanban/#definition","title":"Definition","text":"<p>Kanban is a method for managing and improving workflows using a visual board and incremental process evolution. Originating in lean manufacturing, it has been adapted to software development, analytics, and enterprise planning.</p>"},{"location":"Kanban/#core-principles","title":"Core Principles","text":"<ol> <li> <p>Visualize Work    Make tasks and workflows visible through columns and cards.</p> </li> <li> <p>Limit Work in Progress (WIP)    Restrict the number of items in each stage to avoid overload.</p> </li> <li> <p>Manage Flow    Measure cycle time, identify bottlenecks, and improve throughput.</p> </li> <li> <p>Make Policies Explicit    Define what \u201cdone\u201d means and document process agreements.</p> </li> <li> <p>Implement Feedback Loops    Use daily standups, retrospectives, and metrics for continuous learning.</p> </li> <li> <p>Improve Collaboratively, Evolve Experimentally    Encourage team-led improvements and evolutionary change.</p> </li> </ol>"},{"location":"Kanban/#common-agile-terminology","title":"Common Agile Terminology","text":"Term Agile Definition Agile A mindset and approach emphasizing adaptability and value delivery Scrum A framework based on time-boxed iterations (sprints), with defined roles and ceremonies Kanban A pull-based system for managing and visualizing workflow Sprint A fixed-length iteration (commonly 2\u20134 weeks) where a team delivers a defined set of work User Story A structured expression of user needs (e.g., \u201cAs a [user], I want [goal] so that [value]\u201d) Backlog A prioritized list of tasks or features to be completed Epic A large body of work that can be broken into smaller stories Task A specific action or unit of work, often linked to a user story Card A visual representation of a task or story on a board Board A visual layout of work (columns, swimlanes, cards) Column A workflow state (e.g., To Do, In Progress, Done) WIP Limit Maximum number of items allowed in a column to prevent overload Cycle Time Time taken for a task to move from start to completion Lead Time Time from task request to delivery Velocity The number of story points or tasks completed per sprint (Scrum) Burndown Chart A graph tracking remaining work over time during a sprint Definition of Done Team agreement on what qualifies work as complete Blocked A task that cannot proceed due to a dependency or issue"},{"location":"Kanban/#use-cases-in-utilities-and-it","title":"Use Cases in Utilities and IT","text":"<ul> <li>Prioritizing data quality or stewardship tasks  </li> <li>Managing agile infrastructure and DevOps work  </li> <li>Coordinating SCADA or CMMS enhancement backlogs  </li> <li>Supporting regulatory and audit deliverables  </li> <li>Tracking metadata documentation and integration requests</li> </ul>"},{"location":"Kanban/#sample-kanban-board","title":"Sample Kanban Board","text":"<pre><code>Backlog       | In Progress     | In Review       | Done\n------------------------------------------------------------\nCatalog SCADA | Clean address   | Steward review  | Field forms digitized\nDevelop ETL   | Create glossary | Validate schema | Owner assignment complete\n</code></pre>"},{"location":"Kanban/#benefits","title":"Benefits","text":"<ul> <li>Lightweight and easy to adopt  </li> <li>Encourages team self-organization  </li> <li>Reduces hidden work and multitasking  </li> <li>Provides visibility to stakeholders  </li> <li>Works well with both Agile and non-Agile teams  </li> </ul>"},{"location":"Kanban/#comparisons","title":"Comparisons","text":"Feature Kanban Scrum Work Cadence Continuous flow Iteration-based (sprints) Roles Required None required (flexible) Scrum Master, Product Owner Best For Ops, analytics, shared services Product teams, time-boxed goals Board Type Column-based, flexible Sprint board with backlog Planning Approach Just-in-time (pull-based) Pre-planned sprints"},{"location":"Kanban/#metrics-you-can-track","title":"Metrics You Can Track","text":"<ul> <li>WIP (Work in Progress)  </li> <li>Cycle Time </li> <li>Lead Time </li> <li>Throughput </li> <li>Blocked Rate </li> <li>Aging Work Items</li> </ul>"},{"location":"Kanban/#references-and-further-reading","title":"References and Further Reading","text":"<ul> <li> <p>Kanban: Successful Evolutionary Change for Your Technology Business \u2013 David J. Anderson   ISBN: 9780984521401</p> </li> <li> <p>Essential Kanban Condensed \u2013 David J. Anderson &amp; Andy Carmichael   ISBN: 9780984521425   Free PDF: https://leankanban.com/guide/</p> </li> <li> <p>Making Work Visible \u2013 Dominica DeGrandis   ISBN: 9781788603846   Focuses on hidden work, context switching, and WIP</p> </li> <li> <p>Kanban University   https://www.kanban.university/</p> </li> <li> <p>Atlassian Kanban Guide   https://www.atlassian.com/agile/kanban</p> </li> </ul>"},{"location":"MarkdownGuide/","title":"Markdown Quick Guide","text":"<p>This guide provides a quick reference for using Markdown in MkDocs.</p>"},{"location":"MarkdownGuide/#headings","title":"Headings","text":"<p>Use <code>#</code> for headers, increasing the number for lower levels:</p> <pre><code># H1\n## H2\n### H3\n#### H4\n##### H5\n###### H6\n</code></pre>"},{"location":"MarkdownGuide/#emphasis","title":"Emphasis","text":"<pre><code>*Italic* or _Italic_\n\n**Bold** or __Bold__\n\n***Bold Italic***\n\n~~Strikethrough~~\n</code></pre>"},{"location":"MarkdownGuide/#lists","title":"Lists","text":""},{"location":"MarkdownGuide/#unordered-list","title":"Unordered List","text":"<pre><code>- Item 1\n- Item 2\n  - Nested Item\n* Another bullet\n</code></pre>"},{"location":"MarkdownGuide/#ordered-list","title":"Ordered List","text":"<pre><code>1. First\n2. Second\n   1. Sub-item\n</code></pre>"},{"location":"MarkdownGuide/#links","title":"Links","text":"<pre><code>[Example Link](https://example.com)\n\n[Link with Title](https://example.com \"Optional Title\")\n</code></pre>"},{"location":"MarkdownGuide/#images","title":"Images","text":"<pre><code>![Alt text](https://example.com/image.png \"Optional Title\")\n</code></pre> <p>Local images should be in the same directory or a path relative to the <code>docs/</code> folder.</p>"},{"location":"MarkdownGuide/#inline-code-and-code-blocks","title":"Inline Code and Code Blocks","text":""},{"location":"MarkdownGuide/#inline-code","title":"Inline Code","text":"<p>Use single backticks: <code>`code`</code></p>"},{"location":"MarkdownGuide/#code-block","title":"Code Block","text":"<p>```python</p>"},{"location":"MarkdownGuide/#example","title":"Example","text":"<p>print(\"Hello, MkDocs!\") ```</p>"},{"location":"MarkdownGuide/#blockquotes","title":"Blockquotes","text":"<pre><code>&gt; A simple quote\n\n&gt;&gt; Nested quote\n</code></pre>"},{"location":"MarkdownGuide/#horizontal-rule","title":"Horizontal Rule","text":"<pre><code>---\n</code></pre>"},{"location":"MarkdownGuide/#tables","title":"Tables","text":"<pre><code>| Header 1 | Header 2 |\n|----------|----------|\n| Cell 1   | Cell 2   |\n| Cell 3   | Cell 4   |\n</code></pre>"},{"location":"MarkdownGuide/#task-lists","title":"Task Lists","text":"<p>GitHub-style task lists (works in MkDocs with Material theme):</p> <pre><code>- [x] Task complete\n- [ ] Task incomplete\n</code></pre>"},{"location":"MarkdownGuide/#admonitions","title":"Admonitions","text":"<p>Use for notes, warnings, tips, etc. (Material theme only):</p> <pre><code>!!! note\n    This is a note.\n\n!!! warning\n    This is a warning box.\n</code></pre>"},{"location":"MarkdownGuide/#collapsible-sections","title":"Collapsible Sections","text":"<pre><code>&lt;details&gt;\n&lt;summary&gt;Click to expand&lt;/summary&gt;\n\nThis text is hidden until expanded.\n\n&lt;/details&gt;\n</code></pre>"},{"location":"MarkdownGuide/#html-in-markdown","title":"HTML in Markdown","text":"<p>Inline HTML is supported in most engines:</p> <pre><code>&lt;sup&gt;Superscript&lt;/sup&gt; &lt;kbd&gt;Ctrl&lt;/kbd&gt;+&lt;kbd&gt;C&lt;/kbd&gt;\n</code></pre>"},{"location":"MarkdownGuide/#escaping-characters","title":"Escaping Characters","text":"<p>Use <code>\\</code> to escape special characters:</p> <pre><code>\\*Not italic\\*\n</code></pre>"},{"location":"MarkdownGuide/#footnotes-if-supported","title":"Footnotes (if supported)","text":"<pre><code>Here's a statement.[^1]\n\n[^1]: This is a footnote.\n</code></pre>"},{"location":"MarkdownGuide/#resources","title":"Resources","text":"<ul> <li>CommonMark Help: https://commonmark.org/help/</li> <li>GitHub Markdown Guide: https://guides.github.com/features/mastering-markdown/</li> <li>MkDocs Material Docs: https://squidfunk.github.io/mkdocs-material/</li> </ul>"},{"location":"MedallionArchitecture/","title":"AI/ML Design Guidelines for Medallion Architecture","text":"<p>This guide defines key design principles, implementation techniques, and rationale for embedding machine learning and analytics workflows into a Medallion Architecture using tools such as PostgreSQL, dbt, Dagster, Metabase, and Python.</p>"},{"location":"MedallionArchitecture/#medallion-architecture-overview","title":"Medallion Architecture Overview","text":""},{"location":"MedallionArchitecture/#bronze-layer-raw-data-ingestion","title":"Bronze Layer \u2013 Raw Data Ingestion","text":"<p>Description: This layer ingests raw, unaltered data from source systems\u2014logs, APIs, sensor feeds, files, etc. Target Audience: Data engineers, ETL developers, system integrators</p>"},{"location":"MedallionArchitecture/#silver-layer-cleaned-and-curated-data","title":"Silver Layer \u2013 Cleaned and Curated Data","text":"<p>Description: This layer holds validated, normalized, and joinable data used to build features and perform analytics. Target Audience: Analytics engineers, ML engineers, dashboard developers</p>"},{"location":"MedallionArchitecture/#gold-layer-feature-store-and-analytical-outputs","title":"Gold Layer \u2013 Feature Store and Analytical Outputs","text":"<p>Description: This layer contains enriched datasets, engineered features, predictions, KPIs, and modeling outputs. Target Audience: Data scientists, business analysts, decision-makers, ML ops</p>"},{"location":"MedallionArchitecture/#bronze-layer-raw-ingestion","title":"Bronze Layer \u2013 Raw Ingestion","text":""},{"location":"MedallionArchitecture/#1-time-syncing-and-timestamps","title":"1. Time-Syncing and Timestamps","text":"<p>What: Ensure every record has a consistent, standardized timestamp. How: - Use <code>TIMESTAMP WITH TIME ZONE</code> in PostgreSQL. - Normalize all times to UTC using <code>pendulum</code> or <code>pytz</code>. - Capture both <code>event_time</code> and <code>ingestion_time</code>. Why: Enables accurate joins, time-based aggregations, and leakage-free model training.</p>"},{"location":"MedallionArchitecture/#2-schema-versioning","title":"2. Schema Versioning","text":"<p>What: Track changes to data structure over time. How: - Add a <code>schema_version</code> column. - Log changes in a <code>schema_audit_log</code> table. - Use <code>jsonb</code> columns for semi-structured data. Why: Prevents breakage in downstream pipelines due to silent schema drift.</p>"},{"location":"MedallionArchitecture/#3-ingestion-latency-vs-model-freshness","title":"3. Ingestion Latency vs. Model Freshness","text":"<p>What: Measure the delay between data occurrence and ingestion. How: - Capture <code>ingestion_time</code> at load. - Track <code>ingestion_time - event_time</code> as a latency metric. - Visualize trends in dashboards. Why: Helps assess model freshness and determine retraining intervals.</p>"},{"location":"MedallionArchitecture/#4-source-metadata-and-provenance","title":"4. Source Metadata and Provenance","text":"<p>What: Record where and how each record originated. How: - Add metadata fields: <code>source_name</code>, <code>filename</code>, <code>batch_id</code>, etc. - Store source files in <code>/data/raw/</code> or object storage. - Log ingestion status in a centralized table. Why: Enables audits, traceability, and debugging of unexpected anomalies.</p>"},{"location":"MedallionArchitecture/#silver-layer-cleaned-normalized-and-join-ready","title":"Silver Layer \u2013 Cleaned, Normalized, and Join-Ready","text":""},{"location":"MedallionArchitecture/#1-feature-stability","title":"1. Feature Stability","text":"<p>What: Keep feature definitions consistent across time and models. How: - Enforce value types and allowed sets in dbt or Great Expectations. - Normalize encodings and signs. - Alert on drifted or out-of-bound values. Why: Ensures model training and inference remain stable and predictable.</p>"},{"location":"MedallionArchitecture/#2-unit-normalization","title":"2. Unit Normalization","text":"<p>What: Standardize physical units to ensure comparability. How: - Use the <code>pint</code> library for conversions. - Store raw and normalized values with unit labels. - Maintain a <code>dim_units</code> table for lookup. Why: Avoids data drift and misinterpretation from inconsistent measurement systems.</p>"},{"location":"MedallionArchitecture/#3-missing-value-strategy","title":"3. Missing Value Strategy","text":"<p>What: Define and apply consistent rules for missing values. How: - Add <code>is_&lt;field&gt;_missing</code> flags. - Impute with domain-appropriate logic (mean, zero, ffill, etc.). - Log and version imputation rules. Why: Models need explicit treatment of nulls to avoid unpredictable behavior.</p>"},{"location":"MedallionArchitecture/#4-conformed-dimensions","title":"4. Conformed Dimensions","text":"<p>What: Harmonize shared entities across datasets. How: - Build and maintain <code>dim_customer</code>, <code>dim_location</code>, etc. - Use UUIDs or hashed surrogate keys. - Refresh on schedule via Dagster assets. Why: Supports reusable joins, unified reporting, and integrated ML features.</p>"},{"location":"MedallionArchitecture/#5-as-of-joins","title":"5. As-of Joins","text":"<p>What: Perform joins using only data available up to a specific point in time. How: - Use <code>LATERAL JOIN</code> or windowed <code>ROW_NUMBER()</code> in SQL. - Parameterize timestamp cutoffs for backtesting or training sets. - Wrap in reusable dbt macros. Why: Prevents data leakage by avoiding future-peeking during feature creation.</p>"},{"location":"MedallionArchitecture/#gold-layer-feature-store-and-aggregated-outputs","title":"Gold Layer \u2013 Feature Store and Aggregated Outputs","text":""},{"location":"MedallionArchitecture/#1-feature-versioning","title":"1. Feature Versioning","text":"<p>What: Track changes to feature generation logic. How: - Tag records with <code>feature_version</code>, <code>code_hash</code>, and <code>generated_at</code>. - Use Git to manage transformation scripts. - Register logic and parameters in metadata tables. Why: Enables reproducibility and rollbacks for modeling and governance.</p>"},{"location":"MedallionArchitecture/#2-label-leakage-prevention","title":"2. Label Leakage Prevention","text":"<p>What: Ensure no target label information is accidentally used in training features. How: - Enforce <code>feature_time &lt; label_time</code> logic. - Write automated tests to catch leaks. - Document prediction cutoffs explicitly in code and metadata. Why: Leakage produces misleadingly strong model results and weak production behavior.</p>"},{"location":"MedallionArchitecture/#3-windowed-feature-logic","title":"3. Windowed Feature Logic","text":"<p>What: Use rolling windows to create dynamic, time-sensitive features. How: - Apply <code>pandas.rolling()</code> or <code>dbt_utils.rolling_avg()</code> - Align window end time with prediction timestamps. - Parameterize for A/B testing or seasonal tuning. Why: Captures behavior patterns and trends without introducing future data.</p>"},{"location":"MedallionArchitecture/#4-entity-event-separation","title":"4. Entity-Event Separation","text":"<p>What: Keep static entity info separate from time-series or event logs. How: - Use <code>dim_entity</code> tables and <code>fact_event</code> logs. - Join via <code>entity_id</code> at query or feature build time. - Maintain independent update cadences. Why: Supports flexibility in modeling, schema reuse, and long-term maintenance.</p>"},{"location":"MedallionArchitecture/#5-prediction-audit-trail","title":"5. Prediction Audit Trail","text":"<p>What: Log inference details to support monitoring, accountability, and compliance. How: - Create a <code>prediction_log</code> table with:   - <code>model_version</code>, <code>input_hash</code>, <code>predicted_at</code>, <code>confidence</code>, <code>input_json</code> - Log inference latency and batch IDs. - Store SHAP or LIME explanations alongside results if needed. Why: Enables traceability, fairness audits, and post-hoc validation of model outputs.</p>"},{"location":"MedallionArchitecture/#explainability-reproducibility-enhancements","title":"Explainability &amp; Reproducibility Enhancements","text":""},{"location":"MedallionArchitecture/#shap-shapley-additive-explanations","title":"SHAP (SHapley Additive exPlanations)","text":"<p>What: A game-theoretic method to attribute a prediction to each input feature. How: - Install: <code>pip install shap</code> - Use <code>TreeExplainer</code> or <code>KernelExplainer</code> depending on model type. - Visualize with <code>shap.summary_plot()</code> or <code>force_plot()</code>. Why: Helps data scientists, auditors, and stakeholders understand why a model made a decision.</p>"},{"location":"MedallionArchitecture/#lime-local-interpretable-model-agnostic-explanations","title":"LIME (Local Interpretable Model-Agnostic Explanations)","text":"<p>What: Builds a local, human-interpretable approximation of any model near a single prediction. How: - Install: <code>pip install lime</code> - Use <code>LimeTabularExplainer()</code> with training data - Call <code>.explain_instance()</code> for individual predictions Why: Provides understandable rationales for decisions made by complex or opaque models.</p>"},{"location":"MermaidDiagrams/","title":"Mermaid Diagram Examples","text":"<p>Mermaid diagrams provide lightweight, code-based visuals ideal for use in data governance, analytics, and digital transformation projects. In the water sector, they are particularly useful for documenting SCADA workflows, governance processes, CIP project timelines, asset lifecycle states, and stakeholder communication flows \u2014 all in a version-controlled format that\u2019s easy to embed in MkDocs or GitHub wikis.</p> <p>This reference includes examples of all supported Mermaid diagram types relevant to technical documentation.</p>"},{"location":"MermaidDiagrams/#flowchart","title":"Flowchart","text":"<p>Description: Used for simple process flows, logic structures, or decision diagrams. Ideal for visualizing system logic, workflows, or stakeholder engagement steps in governance.</p> <p>Syntax: <code>flowchart</code></p> <p>Rendered Diagram:</p> <pre><code>flowchart LR\n  A[Start] --&gt; B{SCADA Alarm?}\n  B -- Yes --&gt; C[Send Notification]\n  B -- No --&gt; D[Log Event]\n  C --&gt; E[Close Alert]\n  D --&gt; E</code></pre> <p>Mermaid Code:</p> <pre><code>```mermaid\nflowchart LR\n  A[Start] --&gt; B{SCADA Alarm?}\n  B -- Yes --&gt; C[Send Notification]\n  B -- No --&gt; D[Log Event]\n  C --&gt; E[Close Alert]\n  D --&gt; E\n```\n</code></pre>"},{"location":"MermaidDiagrams/#sequence-diagram","title":"Sequence Diagram","text":"<p>Description: Visualizes interactions between components over time. Useful for illustrating SCADA alert flows or data handoffs across departments.</p> <p>Syntax: <code>sequenceDiagram</code></p> <p>Rendered Diagram:</p> <pre><code>sequenceDiagram\n  participant Operator\n  participant SCADA\n  participant Historian\n  Operator-&gt;&gt;SCADA: Acknowledge alarm\n  SCADA-&gt;&gt;Historian: Log acknowledgment\n  Historian--&gt;&gt;Operator: Status updated</code></pre> <p>Mermaid Code:</p> <pre><code>```mermaid\nsequenceDiagram\n  participant Operator\n  participant SCADA\n  participant Historian\n  Operator-&gt;&gt;SCADA: Acknowledge alarm\n  SCADA-&gt;&gt;Historian: Log acknowledgment\n  Historian--&gt;&gt;Operator: Status updated\n```\n</code></pre>"},{"location":"MermaidDiagrams/#class-diagram","title":"Class Diagram","text":"<p>Description: Represents object-oriented data structures. Ideal for modeling asset hierarchies or telemetry object models.</p> <p>Syntax: <code>classDiagram</code></p> <p>Rendered Diagram:</p> <pre><code>classDiagram\n  Sensor &lt;|-- FlowMeter\n  Sensor &lt;|-- PressureSensor\n  class Sensor {\n    +string id\n    +string type\n    +recordReading()\n  }</code></pre> <p>Mermaid Code:</p> <pre><code>```mermaid\nclassDiagram\n  Sensor &lt;|-- FlowMeter\n  Sensor &lt;|-- PressureSensor\n  class Sensor {\n    +string id\n    +string type\n    +recordReading()\n  }\n```\n</code></pre>"},{"location":"MermaidDiagrams/#state-diagram","title":"State Diagram","text":"<p>Description: Illustrates state transitions. Useful for equipment states (on/off), work order flows, or data validation stages.</p> <p>Syntax: <code>stateDiagram-v2</code></p> <p>Rendered Diagram:</p> <pre><code>stateDiagram-v2\n  [*] --&gt; Requested\n  Requested --&gt; Approved\n  Approved --&gt; Scheduled\n  Scheduled --&gt; Completed\n  Completed --&gt; [*]</code></pre> <p>Mermaid Code:</p> <pre><code>```mermaid\nstateDiagram-v2\n  [*] --&gt; Requested\n  Requested --&gt; Approved\n  Approved --&gt; Scheduled\n  Scheduled --&gt; Completed\n  Completed --&gt; [*]\n```\n</code></pre>"},{"location":"MermaidDiagrams/#entity-relationship-diagram-erd","title":"Entity Relationship Diagram (ERD)","text":"<p>Description: Models database relationships. Ideal for billing systems, AMI integrations, or customer-asset linkages.</p> <p>Syntax: <code>erDiagram</code></p> <p>Rendered Diagram:</p> <pre><code>erDiagram\n  CUSTOMER ||--o{ SERVICE_CONNECTION : has\n  SERVICE_CONNECTION ||--|{ METER : measures\n  CUSTOMER {\n    int id\n    string name\n  }\n  SERVICE_CONNECTION {\n    string location\n    int status\n  }\n  METER {\n    string serial\n    float multiplier\n  }</code></pre> <p>Mermaid Code:</p> <pre><code>```mermaid\nerDiagram\n  CUSTOMER ||--o{ SERVICE_CONNECTION : has\n  SERVICE_CONNECTION ||--|{ METER : measures\n  CUSTOMER {\n    int id\n    string name\n  }\n  SERVICE_CONNECTION {\n    string location\n    int status\n  }\n  METER {\n    string serial\n    float multiplier\n  }\n```\n</code></pre>"},{"location":"MermaidDiagrams/#gantt-chart","title":"Gantt Chart","text":"<p>Description: Displays project timelines and milestones. Common for CIP planning or dashboard rollout tracking.</p> <p>Syntax: <code>gantt</code></p> <p>Rendered Diagram:</p> <pre><code>gantt\n  title Smart Meter Rollout\n  dateFormat YYYY-MM-DD\n  section Planning\n  Requirements :a1, 2025-01-01, 10d\n  section Deployment\n  District A :a2, after a1, 15d\n  District B :a3, after a2, 15d\n  section Reporting\n  Summary Reports :a4, after a3, 5d</code></pre> <p>Mermaid Code:</p> <pre><code>```mermaid\ngantt\n  title Smart Meter Rollout\n  dateFormat YYYY-MM-DD\n  section Planning\n  Requirements :a1, 2025-01-01, 10d\n  section Deployment\n  District A :a2, after a1, 15d\n  District B :a3, after a2, 15d\n  section Reporting\n  Summary Reports :a4, after a3, 5d\n```\n</code></pre>"},{"location":"MermaidDiagrams/#pie-chart","title":"Pie Chart","text":"<p>Description: Visualizes simple proportion breakdowns (e.g., usage by sector, cost by category).</p> <p>Syntax: <code>pie</code></p> <p>Rendered Diagram:</p> <pre><code>pie\ntitle Water Demand by Sector\n\"Residential\" : 50\n\"Commercial\" : 20\n\"Industrial\" : 10\n\"Agricultural\" : 20</code></pre> <p>Mermaid Code:</p> <pre><code>```mermaid\npie\ntitle Water Demand by Sector\n\"Residential\" : 50\n\"Commercial\" : 20\n\"Industrial\" : 10\n\"Agricultural\" : 20\n```\n</code></pre>"},{"location":"MermaidDiagrams/#git-graph","title":"Git Graph","text":"<p>Description: Models Git workflow. Useful for documenting ETL pipelines or dashboard versioning.</p> <p>Syntax: <code>gitGraph</code></p> <p>Rendered Diagram:</p> <pre><code>gitGraph\n  commit\n  commit\n  branch dev\n  checkout dev\n  commit\n  checkout main\n  merge dev</code></pre> <p>Mermaid Code:</p> <pre><code>```mermaid\ngitGraph\n  commit\n  commit\n  branch dev\n  checkout dev\n  commit\n  checkout main\n  merge dev\n```\n</code></pre>"},{"location":"MermaidDiagrams/#timeline","title":"Timeline","text":"<p>Description: Used for plotting key events chronologically (e.g., audits, go-lives).</p> <p>Syntax: <code>timeline</code></p> <p>Rendered Diagram:</p> <pre><code>timeline\n  title AMI Deployment Timeline\n  2024-01 : Kickoff\n  2024-02 : Site Surveys\n  2024-05 : Installations Start\n  2024-09 : Full Go-Live</code></pre> <p>Mermaid Code:</p> <pre><code>```mermaid\ntimeline\n  title AMI Deployment Timeline\n  2024-01 : Kickoff\n  2024-02 : Site Surveys\n  2024-05 : Installations Start\n  2024-09 : Full Go-Live\n```\n</code></pre>"},{"location":"MermaidDiagrams/#journey-user-journey","title":"Journey (User Journey)","text":"<p>Description: Maps user steps and emotional states across a process. Great for CX design or customer billing feedback.</p> <p>Syntax: <code>journey</code></p> <p>Rendered Diagram:</p> <pre><code>journey\n  title New Customer Onboarding\n  section Website\n    Register: 5: User\n    Confirm Email: 4: User\n  section Account Setup\n    Add Service Address: 3: User\n    Link Meter: 2: User</code></pre> <p>Mermaid Code:</p> <pre><code>```mermaid\njourney\n  title New Customer Onboarding\n  section Website\n    Register: 5: User\n    Confirm Email: 4: User\n  section Account Setup\n    Add Service Address: 3: User\n    Link Meter: 2: User\n```\n</code></pre>"},{"location":"NISTCSF/","title":"NIST Cybersecurity Framework (CSF) Guide","text":"<p>The NIST Cybersecurity Framework (CSF) provides a flexible, repeatable, and cost-effective approach for managing cybersecurity risk. It is widely adopted in critical infrastructure, utilities, and IT/OT environments.</p> <p>Originally published by NIST in 2014 and updated in 2018 (v1.1) and 2024 (v2.0), the CSF consists of five Core Functions, further organized into Categories and Subcategories.</p>"},{"location":"NISTCSF/#csf-core-functions-overview","title":"\ud83d\udd37 CSF Core Functions Overview","text":"Function Purpose Identify Understand organizational context, risk, and assets Protect Implement safeguards to ensure service delivery Detect Discover cybersecurity events in time Respond Take action during or after a detected event Recover Restore capabilities and services after disruption"},{"location":"NISTCSF/#csf-categories-by-function","title":"\ud83d\udcc2 CSF Categories by Function","text":""},{"location":"NISTCSF/#identify","title":"\ud83d\udfe6 Identify","text":"Category Description Asset Management Inventory physical and digital assets Governance Define risk management roles and policies Risk Assessment Understand threats, vulnerabilities Supply Chain Risk Management Identify third-party dependencies"},{"location":"NISTCSF/#protect","title":"\ud83d\udfe9 Protect","text":"Category Description Identity Management Control access to systems and data Awareness &amp; Training Educate staff and stakeholders Data Security Protect data at rest and in transit Protective Technology Harden endpoints and network infrastructure"},{"location":"NISTCSF/#detect","title":"\ud83d\udfe8 Detect","text":"Category Description Anomalies &amp; Events Monitor for deviations from normal behavior Continuous Monitoring Use tools to track assets and risks Detection Processes Define playbooks and alerting protocols"},{"location":"NISTCSF/#respond","title":"\ud83d\udfe5 Respond","text":"Category Description Response Planning Establish incident response processes Communications Coordinate with stakeholders and public Analysis Investigate impact and root cause Mitigation Contain the incident Improvements Update response plans and security controls"},{"location":"NISTCSF/#recover","title":"\ud83d\udfea Recover","text":"Category Description Recovery Planning Maintain recovery procedures and backups Improvements Learn from incidents and strengthen posture Communications Coordinate internal and external messaging"},{"location":"NISTCSF/#implementation-tiers","title":"\ud83d\udcca Implementation Tiers","text":"<p>Tiers describe how an organization manages cybersecurity risk. They are not maturity levels but help assess current capabilities.</p> Tier Description 1 Partial \u2013 Ad hoc, reactive 2 Risk Informed \u2013 Some risk processes exist 3 Repeatable \u2013 Formalized and consistent 4 Adaptive \u2013 Continuous improvement based on lessons learned"},{"location":"NISTCSF/#utilityot-mapping-examples","title":"\ud83c\udfed Utility/OT Mapping Examples","text":"CSF Function Example for Water/Wastewater Utility Identify Inventory PLCs, SCADA servers, and telemetry nodes Protect Role-based access to HMI terminals; VLAN isolation Detect Use SCADA anomaly detection or IDS for OT traffic Respond OT incident response runbooks for ICS downtime Recover Restore SCADA configs and test system integrity"},{"location":"NISTCSF/#common-tools-by-function","title":"\ud83e\uddf0 Common Tools by Function","text":"Function Tools &amp; Technologies Identify CMDB, OT asset discovery (Nozomi, Claroty), NIST RMF Protect RBAC, MFA, network segmentation, VPN, firewall Detect SIEM, OT IDS, log correlation, endpoint detection (EDR) Respond SOAR, IR playbooks, ticketing (ServiceNow, Jira), audit logs Recover Immutable backups, BCP plans, system reimage scripts"},{"location":"NISTCSF/#nist-csf-yaml-for-automation-or-config-as-doc","title":"\ud83d\udcd8 NIST CSF YAML (for automation or config-as-doc)","text":"<p>```yaml csf:   functions:     identify:       - asset_management       - governance       - risk_assessment       - supply_chain_risk_management     protect:       - identity_management       - awareness_training       - data_security       - protective_technology     detect:       - anomalies_and_events       - continuous_monitoring       - detection_processes     respond:       - response_planning       - communications       - analysis       - mitigation       - improvements     recover:       - recovery_planning       - improvements       - communications   tiers:     1: Partial     2: Risk Informed     3: Repeatable     4: Adaptive</p>"},{"location":"NISTGuide/","title":"NIST SP 800-30 Rev. 1: Guide for Conducting Risk Assessments","text":"<p>NIST Special Publication 800-30 Revision 1 provides a structured methodology for conducting risk assessments within organizations. It is a critical component of the broader Risk Management Framework (RMF) and aligns with NIST SP 800-39, which outlines the overarching risk management process.</p>"},{"location":"NISTGuide/#purpose","title":"\ud83d\udcd8 Purpose","text":"<p>The guide aims to assist organizations in:</p> <ul> <li>Identifying potential threats to information systems.</li> <li>Assessing vulnerabilities and the potential impact of threats.</li> <li>Determining the likelihood of adverse events.</li> <li>Evaluating the level of risk to organizational operations and assets.</li> <li>Informing risk response decisions and prioritizing security measures.</li> </ul>"},{"location":"NISTGuide/#risk-management-process-overview","title":"\ud83e\udded Risk Management Process Overview","text":"<p>NIST SP 800-30 integrates into the four-step risk management process defined in NIST SP 800-39:</p> <ol> <li>Frame Risk: Establish the context for risk-based decisions.</li> <li>Assess Risk: Identify and evaluate risks to organizational operations.</li> <li>Respond to Risk: Develop and implement strategies to mitigate identified risks.</li> <li>Monitor Risk: Continuously oversee risk factors and the effectiveness of risk responses.</li> </ol>"},{"location":"NISTGuide/#risk-assessment-process","title":"\ud83d\udee0\ufe0f Risk Assessment Process","text":"<p>The risk assessment process comprises four key steps:</p>"},{"location":"NISTGuide/#1-prepare-for-the-assessment","title":"1. Prepare for the Assessment","text":"<ul> <li>Define Purpose: Clarify the objectives and intended outcomes.</li> <li>Determine Scope: Identify the systems, processes, or organizations involved.</li> <li>Identify Assumptions and Constraints: Acknowledge any limitations or predefined conditions.</li> <li>Select Risk Model and Methodology: Choose appropriate frameworks and tools for assessment.</li> </ul>"},{"location":"NISTGuide/#2-conduct-the-assessment","title":"2. Conduct the Assessment","text":"<ul> <li>Identify Threat Sources and Events: Recognize potential adversaries and non-adversarial events.</li> <li>Identify Vulnerabilities and Predisposing Conditions: Determine weaknesses that could be exploited.</li> <li>Determine Likelihood: Assess the probability of threat events occurring.</li> <li>Determine Impact: Evaluate the potential consequences of threat events.</li> <li>Determine Risk: Combine likelihood and impact to ascertain risk levels.</li> </ul>"},{"location":"NISTGuide/#3-communicate-results","title":"3. Communicate Results","text":"<ul> <li>Document Findings: Compile a comprehensive report detailing risks and assessment methodologies.</li> <li>Share Information: Disseminate results to stakeholders to inform decision-making.</li> </ul>"},{"location":"NISTGuide/#4-maintain-the-assessment","title":"4. Maintain the Assessment","text":"<ul> <li>Update Regularly: Reassess risks periodically or when significant changes occur.</li> <li>Monitor Changes: Stay informed about new threats, vulnerabilities, and organizational changes.</li> </ul>"},{"location":"NISTGuide/#risk-management-tiers","title":"\ud83c\udfe2 Risk Management Tiers","text":"<p>NIST SP 800-30 emphasizes a three-tiered approach to risk management:</p> <ul> <li>Tier 1: Organization Level</li> <li>Focuses on governance, risk management strategies, and organizational policies.</li> <li>Tier 2: Mission/Business Process Level</li> <li>Addresses risks to specific missions or business functions.</li> <li>Tier 3: Information System Level</li> <li>Concentrates on risks to individual information systems and their components.</li> </ul>"},{"location":"NISTGuide/#risk-factors-and-models","title":"\ud83d\udcca Risk Factors and Models","text":"<p>Risk assessments consider the following factors:</p> <ul> <li>Threat Sources: Originators of potential adverse events (e.g., hackers, natural disasters).</li> <li>Threat Events: Specific incidents that could cause harm.</li> <li>Vulnerabilities: Weaknesses that could be exploited by threats.</li> <li>Predisposing Conditions: Existing conditions that may increase susceptibility to threats.</li> <li>Likelihood: Probability that a threat event will occur.</li> <li>Impact: Potential consequences or damages resulting from a threat event.</li> </ul> <p>Risk is typically calculated as:</p>"},{"location":"NISTGuide/#risk-assessment-report-components","title":"\ud83d\udcc4 Risk Assessment Report Components","text":"<p>A comprehensive risk assessment report should include:</p> <ul> <li>Executive Summary: Overview of findings and recommendations.</li> <li>Assessment Methodology: Description of the processes and tools used.</li> <li>Detailed Findings: In-depth analysis of identified risks.</li> <li>Risk Evaluation: Assessment of risk levels and prioritization.</li> <li>Recommendations: Suggested actions for risk mitigation.</li> <li>Appendices: Supporting data, references, and detailed analyses.</li> </ul>"},{"location":"NISTGuide/#references","title":"\ud83d\udd17 References","text":"<ul> <li>NIST SP 800-30 Rev. 1 - Guide for Conducting Risk Assessments</li> <li>NIST SP 800-39 - Managing Information Security Risk</li> <li>NIST Risk Management Framework</li> </ul>"},{"location":"NISTRMF/","title":"NIST Risk Management Framework (RMF) Guide","text":"<p>The NIST Risk Management Framework (RMF) is a flexible, repeatable process that helps organizations manage cybersecurity and privacy risks for information systems. Defined in NIST SP 800-37 Rev. 2, the RMF integrates with enterprise governance and promotes continuous monitoring and authorization.</p>"},{"location":"NISTRMF/#purpose","title":"\ud83d\udcd8 Purpose","text":"<p>RMF enables organizations to:</p> <ul> <li>Make informed, risk-based decisions</li> <li>Integrate cybersecurity into the system development life cycle (SDLC)</li> <li>Demonstrate compliance with FISMA, FedRAMP, and other NIST 800-series standards</li> <li>Align operational risks with organizational objectives</li> </ul>"},{"location":"NISTRMF/#rmf-steps","title":"\ud83d\udd01 RMF Steps","text":"Step Name Description 0 Prepare Define roles, context, resources, and risk appetite 1 Categorize Assign impact levels to system functions and data using FIPS 199 2 Select Choose baseline controls from NIST SP 800-53 3 Implement Deploy and document selected security controls 4 Assess Evaluate controls using NIST SP 800-53A 5 Authorize Make a risk-based decision and issue ATO, IATO, or denial 6 Monitor Continuously assess security posture and control effectiveness"},{"location":"NISTRMF/#step-by-step-details","title":"\ud83e\udded Step-by-Step Details","text":""},{"location":"NISTRMF/#step-0-prepare","title":"Step 0: Prepare","text":"<ul> <li>Define RMF participants (AO, ISSO, system owner)</li> <li>Identify common controls and shared services</li> <li>Understand business context and organizational risk tolerance</li> <li>Link cybersecurity to mission/business objectives</li> </ul>"},{"location":"NISTRMF/#step-1-categorize","title":"Step 1: Categorize","text":"<ul> <li>Follow FIPS 199 and NIST SP 800-60</li> <li>Evaluate impact on Confidentiality, Integrity, Availability (CIA)</li> <li>Output: System Categorization Document</li> </ul>"},{"location":"NISTRMF/#step-2-select","title":"Step 2: Select","text":"<ul> <li>Use NIST SP 800-53 Rev. 5 control baselines</li> <li>Tailor baseline (Low/Moderate/High) using overlays</li> <li>Output: System Security Plan (SSP)</li> </ul>"},{"location":"NISTRMF/#step-3-implement","title":"Step 3: Implement","text":"<ul> <li>Configure controls and document implementation</li> <li>Use automated configuration management (e.g., Ansible, Terraform)</li> <li>Output: Updated SSP with implementation details</li> </ul>"},{"location":"NISTRMF/#step-4-assess","title":"Step 4: Assess","text":"<ul> <li>Use NIST SP 800-53A Rev. 5</li> <li>Evaluate if controls are implemented correctly and producing expected results</li> <li>Output: Security Assessment Report (SAR)</li> </ul>"},{"location":"NISTRMF/#step-5-authorize","title":"Step 5: Authorize","text":"<ul> <li>Review SSP, SAR, and POA&amp;M</li> <li>Authorizing Official (AO) accepts, rejects, or delays system use</li> <li>Decision types: ATO (Authorize to Operate), IATO (Interim), or Denial</li> </ul>"},{"location":"NISTRMF/#step-6-monitor","title":"Step 6: Monitor","text":"<ul> <li>Establish continuous monitoring strategy (ConMon)</li> <li>Use log management, SIEM, vulnerability scanning, and config monitoring</li> <li>Keep SSP, POA&amp;M, and SAR updated</li> </ul>"},{"location":"NISTRMF/#core-roles","title":"\ud83d\udc65 Core Roles","text":"Role Responsibility Authorizing Official (AO) Makes the final risk-based decision to authorize System Owner Manages system and ensures security integration ISSO Day-to-day security oversight of the system Security Control Assessor Performs control evaluations (internal or third party) Risk Executive Function Aligns system risks with organizational priorities"},{"location":"NISTRMF/#control-sources-guidance","title":"\ud83d\udcda Control Sources &amp; Guidance","text":"Document Role in RMF NIST SP 800-37 Rev. 2 Defines the RMF process NIST SP 800-53 Rev. 5 Security and privacy control catalog NIST SP 800-53A Assessment procedures for controls FIPS 199 Categorization of system impact levels FIPS 200 Minimum security requirements NIST SP 800-60 Vol. 1 Mapping of data types to impact categories"},{"location":"NISTRMF/#rmf-in-practice","title":"\ud83c\udfed RMF in Practice","text":"Use Case Notes FedRAMP RMF is adapted with specific baselines and assessment templates. See FedRAMP.gov OT/ICS Systems Pair RMF with NIST SP 800-82 for ICS/SCADA environments DevSecOps Embed RMF artifacts into CI/CD pipelines with IaC and container security"},{"location":"NISTRMF/#rmf-yaml-reference-block","title":"\ud83d\udcd8 RMF YAML Reference Block","text":"<p>```yaml nist_rmf:   steps:     0: prepare     1: categorize     2: select     3: implement     4: assess     5: authorize     6: monitor   artifacts:     - system_security_plan     - security_assessment_report     - plan_of_action_and_milestones     - authorization_package   controls_catalog: nist_sp_800_53_rev_5   assessment_guidance: nist_sp_800_53a   roles:     - authorizing_official     - system_owner     - isso     - risk_executive_function     - security_control_assessor   outputs:     - impact_levels: [low, moderate, high]     - authorizations: [ATO, IATO, Denied]</p>"},{"location":"OKROverview/","title":"OKR Essentials Guide","text":"<p>A practical introduction to Objectives and Key Results (OKRs), based on the methodology promoted by WhatMatters.com.</p>"},{"location":"OKROverview/#what-are-okrs","title":"\ud83d\udccc What Are OKRs?","text":"<p>OKRs (Objectives and Key Results) are a structured, transparent goal-setting framework used by organizations to align effort, drive focus, and measure outcomes.</p> <ul> <li>Objective: A clearly defined, qualitative goal that inspires and provides direction.</li> <li>Key Results: A set of measurable outcomes that indicate progress toward achieving the Objective.</li> </ul>"},{"location":"OKROverview/#why-use-okrs","title":"\ud83e\udded Why Use OKRs?","text":"<p>OKRs help organizations shift from reactive to strategic execution by:</p> <ul> <li>Aligning teams around common goals, eliminating duplication or misaligned efforts.</li> <li>Creating transparency, so everyone understands what others are working toward.</li> <li>Focusing on outcomes, not just activity or output.</li> <li>Driving performance by encouraging stretch goals and progress monitoring.</li> <li>Enabling agility through regular reassessment and reprioritization of goals.</li> </ul>"},{"location":"OKROverview/#okr-structure","title":"\ud83e\uddf1 OKR Structure","text":""},{"location":"OKROverview/#objective-o","title":"\ud83c\udfaf Objective (O)","text":"<p>The \u201cWhat\u201d \u2014 a bold, qualitative statement of intent.</p> <ul> <li>Inspirational and time-bound</li> <li>Focused on outcomes rather than outputs</li> <li>Should answer: Where do we want to go?</li> </ul> <p>Example: Improve customer satisfaction across all support channels</p>"},{"location":"OKROverview/#key-results-krs","title":"\ud83d\udccf Key Results (KRs)","text":"<p>The \u201cHow\u201d \u2014 specific, numeric targets that define success for the objective.</p> <ul> <li>Quantifiable and time-constrained</li> <li>Typically 2\u20135 per objective</li> <li>Should answer: How will we measure success?</li> </ul> <p>Example KRs: - Increase Net Promoter Score (NPS) from 52 to 65 - Reduce average support response time from 24 to 8 hours - Achieve 95% customer satisfaction in support surveys  </p>"},{"location":"OKROverview/#best-practices-for-okrs","title":"\ud83e\udde0 Best Practices for OKRs","text":"<ul> <li>Keep it simple: Limit to 3\u20135 OKRs per team to maintain clarity and focus.</li> <li>Stretch, but don\u2019t break: Aim for ambitious goals that challenge the team, but remain within reach.</li> <li>Align with purpose: OKRs should clearly support the organization\u2019s mission and strategic objectives.</li> <li>Empower teams: Allow teams to write their own OKRs aligned to higher-level goals\u2014don\u2019t just cascade top-down.</li> <li>Separate from compensation: OKRs are for learning and alignment, not evaluation or bonuses.</li> <li>Make them visible: Share OKRs across the organization to promote transparency and mutual support.</li> <li>Review and update often: OKRs are living tools; they should be checked in on weekly and reviewed quarterly.</li> </ul>"},{"location":"OKROverview/#the-okr-cycle","title":"\ud83d\udd01 The OKR Cycle","text":"Phase Description Set Define OKRs at the start of a cycle (quarterly is common); gather cross-functional input. Align Ensure OKRs support higher-level objectives and are shared across teams to reduce duplication. Track Monitor weekly or bi-weekly progress using check-ins, dashboards, or status updates. Reflect At the end of the cycle, assess results, score key results, and document lessons learned."},{"location":"OKROverview/#scoring-okrs-optional","title":"\ud83e\uddee Scoring OKRs (Optional)","text":"<p>A typical scoring range is from 0.0 to 1.0 per key result:</p> <ul> <li>1.0 = Fully achieved  </li> <li>0.7\u20130.9 = Substantial progress, but not complete (ideal target zone)  </li> <li>&lt; 0.4 = Fell short; may indicate overreach, lack of focus, or external blockers  </li> </ul> <p>A team consistently scoring 0.6\u20130.7 is likely balancing ambition and execution well.</p> <p>Scoring should be used for learning and prioritization\u2014not performance management.</p>"},{"location":"OKROverview/#common-pitfalls","title":"\ud83d\udeab Common Pitfalls","text":"<ul> <li>Too many OKRs: Leads to dilution of effort and lack of focus.</li> <li>Vague or immeasurable KRs: Makes tracking progress impossible.</li> <li>Confusing outputs with outcomes: Focus on impact, not tasks completed.</li> <li>Set-and-forget mentality: OKRs must be reviewed frequently to remain relevant.</li> <li>Using OKRs for compensation: This undermines risk-taking and transparency.</li> </ul>"},{"location":"OKROverview/#resources","title":"\ud83d\udcda Resources","text":"<ul> <li>WhatMatters.com \u2013 Official OKR resource hub with case studies and tools  </li> <li>Measure What Matters by John Doerr \u2013 Foundational book introducing OKRs  </li> <li>Google Re:Work \u2013 Guidance from Google\u2019s early OKR experience  </li> <li>Atlassian Team Playbook \u2013 Includes OKR facilitation techniques</li> </ul>"},{"location":"OKROverview/#quick-okr-checklist","title":"\u2705 Quick OKR Checklist","text":"<p>Use this list to validate whether your OKRs are well-formed and useful:</p> <ul> <li> Does the Objective describe a meaningful outcome?</li> <li> Are the Key Results measurable, time-bound, and outcome-focused?</li> <li> Are OKRs ambitious yet realistic?</li> <li> Are they aligned with organizational or team strategy?</li> <li> Are progress and blockers reviewed weekly or biweekly?</li> <li> Are results reflected upon and shared at the end of the cycle?</li> </ul>"},{"location":"PerformanceTerms/","title":"Metrics vs KPIs vs Measures vs Key Results","text":"<p>This page defines and distinguishes between performance terms used in data governance, OKRs, and operational monitoring, grouped by strategic and tactical function.</p>"},{"location":"PerformanceTerms/#strategic-layer-vision-goals-and-outcomes","title":"Strategic Layer: Vision, Goals, and Outcomes","text":""},{"location":"PerformanceTerms/#vision","title":"Vision","text":"<p>A long-term aspirational statement defining what the organization ultimately seeks to achieve. Example: \u201cTo deliver clean, resilient water services for future generations.\u201d</p>"},{"location":"PerformanceTerms/#mission","title":"Mission","text":"<p>The organization\u2019s core purpose and guiding philosophy. Example: \u201cTo operate and maintain water infrastructure that protects public health, meets regulatory requirements, and delivers exceptional value to our community.\u201d</p>"},{"location":"PerformanceTerms/#north-star-objective","title":"North Star Objective","text":"<p>A unifying, long-range goal that connects all teams and initiatives. Example: \u201cBuild a data-driven utility that empowers transparent decision-making, proactive asset management, and exceptional service delivery.\u201d</p>"},{"location":"PerformanceTerms/#objectives-in-okrs","title":"Objectives (in OKRs)","text":"<p>Aspirational, qualitative statements of what we want to achieve.</p> <p>Characteristics:</p> <ul> <li>Time-bound (quarterly or annual)</li> <li>Aligned with mission</li> <li>Inspirational, not numerical</li> <li>Supported by Key Results</li> </ul> <p>Examples:</p> <ul> <li>\u201cImprove system reliability across all treatment facilities\u201d</li> <li>\u201cAccelerate digital modernization of field operations\u201d</li> </ul>"},{"location":"PerformanceTerms/#key-results","title":"Key Results","text":"<p>Measurable, time-bound outcomes that define success for an Objective.</p> <p>Characteristics:</p> <ul> <li>Quantitative</li> <li>Outcome-focused</li> <li>Time-boxed and scored</li> </ul> <p>Examples:</p> <ul> <li>\u201cReduce unplanned downtime by 50% by end of Q3\u201d</li> <li>\u201cDigitize 100% of field forms by Q4\u201d</li> </ul>"},{"location":"PerformanceTerms/#key-performance-indicators-kpis","title":"Key Performance Indicators (KPIs)","text":"<p>High-priority metrics used to evaluate performance toward organizational goals.</p> <p>Characteristics:</p> <ul> <li>Tied to strategic priorities</li> <li>Have thresholds or targets</li> <li>Featured in dashboards</li> </ul> <p>Examples:</p> <ul> <li>Average time to repair (&lt; 24 hrs)</li> <li>Water loss percentage (&lt; 10%)</li> <li>Customer satisfaction (&gt; 90%)</li> </ul>"},{"location":"PerformanceTerms/#tactical-layer-data-inputs-and-monitoring-tools","title":"Tactical Layer: Data Inputs and Monitoring Tools","text":""},{"location":"PerformanceTerms/#metrics","title":"Metrics","text":"<p>Quantitative values used to monitor status of processes or systems.</p> <p>Characteristics:</p> <ul> <li>Descriptive</li> <li>Operational or strategic</li> <li>Collected continuously</li> </ul> <p>Examples:</p> <ul> <li>Gallons treated per day</li> <li>Service requests received</li> <li>Page load time</li> </ul>"},{"location":"PerformanceTerms/#measures","title":"Measures","text":"<p>The smallest units of quantifiable observation. Often inputs to metrics and KPIs.</p> <p>Characteristics:</p> <ul> <li>Fine-grained</li> <li>Used in calculations</li> <li>Require context to be meaningful</li> </ul> <p>Examples:</p> <ul> <li>Number of outages</li> <li>Uptime hours</li> <li>Cost per treatment unit</li> </ul>"},{"location":"PerformanceTerms/#summary-comparison","title":"Summary Comparison","text":"Term Layer Granularity Goal Link Example Vision Strategic N/A Aspirational \u201cClean, resilient water for future gens\u201d Mission Strategic N/A Directional \u201cOperate safe, compliant systems\u201d North Star Strategic N/A Unifying \u201cEmpower decision-making with data\u201d Objective Strategic Broad Yes \u201cModernize field ops\u201d Key Result Strategic Focused Always \u201cDigitize 100% of forms by Q4\u201d KPI Strategic Focused Always \u201c&lt; 24 hrs to repair\u201d Metric Tactical Moderate Sometimes \u201cDowntime in hours\u201d Measure Tactical Fine Not required \u201cCost per treatment unit\u201d"},{"location":"PerformanceTerms/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Strategy defines the vision and goals; tactics measure and monitor progress.</li> <li>Objectives tell us what we want to achieve; Key Results show if we\u2019ve achieved it.</li> <li>KPIs help track performance over time; metrics and measures provide raw insights.</li> <li>The North Star Objective gives durable direction across teams and OKRs.</li> </ul>"},{"location":"PurdueModel/","title":"Purdue Model for Industrial Control Systems (ICS)","text":"<p>The Purdue Model (Purdue Enterprise Reference Architecture) is a layered architecture framework that defines how information technology (IT) and operational technology (OT) systems interact in industrial environments. It provides a reference for securing, segmenting, and modernizing systems used in manufacturing, utilities, and critical infrastructure.</p>"},{"location":"PurdueModel/#purdue-model-overview","title":"Purdue Model Overview","text":"Level Function Scope Common Examples Level 5 Enterprise Network (IT) Corporate applications and cloud ERP, CRM, M365, BI Tools, Cloud Storage Level 4 Business Logistics Systems Site-wide operations and analytics MES, CMMS, LIMS, Historian, Data Lake Level 3 Site Operations / DMZ Aggregation and security bridge SCADA servers, OPC-UA, MQTT Broker, Firewall Level 2 Area Supervisory Control Local control room functions SCADA HMIs, Historian Nodes, Batch Control Level 1 Basic Control Machine-level instructions PLCs, RTUs, PACs, Motor Controllers Level 0 Physical Process Sensors and actuators Flow meters, temperature sensors, valves"},{"location":"PurdueModel/#modern-enhancements-to-purdue-model","title":"Modern Enhancements to Purdue Model","text":"Enhancement Placement Description Edge Compute / Gateway Levels 1\u20133 Local preprocessing of OT data before transmission Unified Namespace (UNS) Level 3.5 MQTT or event-driven pub/sub model connecting OT to IT systems Cloud Integration Levels 4\u20135 Data lakes, dashboards, AI/ML models, centralized reporting Zero Trust Security Levels 3\u20135 Role-based access, network segmentation, encrypted northbound flows Data Diodes / Brokers Level 3.5 One-way data transmission from OT to IT"},{"location":"PurdueModel/#ics-data-flow","title":"ICS Data Flow","text":"<p>```yaml purdue_model:   levels:     level_5:       name: Enterprise Network (IT)       function: Corporate applications and cloud services       examples:         - ERP         - CRM         - Power BI         - M365         - Cloud Storage     level_4:       name: Business Logistics Systems       function: Site-wide operations, reporting, analytics       examples:         - MES         - CMMS         - LIMS         - Historian         - Data Lake     level_3:       name: Site Operations / DMZ       function: Aggregation of SCADA data, secure IT/OT boundary       examples:         - SCADA servers         - OPC-UA         - MQTT Broker         - Firewalls     level_2:       name: Area Supervisory Control       function: Local control room, process monitoring       examples:         - SCADA HMI         - Historian nodes         - Batch controllers     level_1:       name: Basic Control       function: Machine-level automation and instruction execution       examples:         - PLCs         - RTUs         - PACs     level_0:       name: Physical Process       function: Sensor data collection and actuator control       examples:         - Flow meters         - Actuators         - Temperature sensors         - Valves</p> <p>enhancements:     - name: Edge Compute       placement: L1\u2013L3       description: Local preprocessing and buffering of OT data     - name: Unified Namespace (UNS)       placement: L3.5       description: Real-time event-driven MQTT pub/sub architecture     - name: Cloud Integration       placement: L4\u2013L5       description: Enables cloud-based BI, AI/ML, and enterprise dashboards     - name: Zero Trust Architecture       placement: L3\u2013L5       description: Enforces fine-grained access and traffic segmentation     - name: Data Diode / Broker       placement: L3.5       description: One-way transmission from OT to IT</p> <p>security_zones:     - zone: Enterprise       levels: [5]       description: Business applications and cloud interfaces     - zone: Industrial DMZ       levels: [3.5]       description: Segregates OT from IT; monitored bridge layer     - zone: Manufacturing       levels: [2, 3]       description: Control room operations, SCADA visualization     - zone: Control       levels: [0, 1]       description: Field-level, real-time deterministic control</p> <p>use_cases:     - name: Predictive Maintenance       tools: [MQTT, InfluxDB, MLflow, Power BI]       levels_spanned: [1, 2, 3, 4, 5]     - name: SCADA Modernization       tools: [Node-RED, MQTT, Grafana]       levels_spanned: [2, 3, 4, 5]     - name: Regulatory Reporting       tools: [Historian, dbt, Superset]       levels_spanned: [3, 4, 5]     - name: Leak Detection       tools: [OPC-UA, Edge AI, Cloud Alert]       levels_spanned: [0, 1, 3, 4]     - name: Energy Optimization       tools: [Smart Metering, MES, BI Dashboard]       levels_spanned: [0, 1, 4, 5]</p> <p>references:     - https://csrc.nist.gov/publications/detail/sp/800-82/rev-2/final     - https://www.isa.org/standards-and-publications/isa-standards/isa-62443-series-of-standards     - https://inductiveautomation.com/resources/article/what-is-the-purdue-model     - https://docs.aws.amazon.com/whitepapers/latest/aws-industrial-iot/aws-industrial-iot.html     - https://www.microsoft.com/security/blog/2021/10/27/zero-trust-for-ot/</p>"},{"location":"RPi5/","title":"Project Stack","text":"<p>This project showcases a modular, low-cost data analytics and observability stack deployed on a Raspberry Pi 5 running Pi OS Lite (Bookworm). It reflects real-world architecture patterns for edge analytics and demonstrates a fully open-source solution for data lineage, transformation, monitoring, and governance.</p>"},{"location":"RPi5/#rpi5-stack","title":"RPi5 Stack","text":""},{"location":"RPi5/#raspberry-pi-os-lite","title":"Raspberry Pi OS Lite","text":"<p>A minimal Debian-based distribution selected for its headless, lightweight footprint. It is flashed to a microSD card and accessed via SSH to emulate edge deployment conditions.</p>"},{"location":"RPi5/#visual-studio-code-remote","title":"Visual Studio Code (Remote)","text":"<p>Remote SSH integration with VS Code allows the developer to manage the Pi from a desktop environment. This supports inline Git usage, terminal access, and Python development directly on the target system.</p>"},{"location":"RPi5/#git","title":"Git","text":"<p>All scripts, configurations, and dashboards are tracked in a Git repository. SSH keys manage secure access, enabling rollback, collaboration, and infrastructure versioning.</p>"},{"location":"RPi5/#python-venv","title":"Python (venv)","text":"<p>Used throughout for ETL scripting, orchestration, and integration. A dedicated virtual environment (<code>~/scr/venv</code>) contains core libraries like <code>polars</code>, <code>sqlalchemy</code>, and <code>pint</code>.</p>"},{"location":"RPi5/#duckdb","title":"DuckDB","text":"<p>Serves as the OLAP engine for analytical queries over the silver-tier data. It is optimized for local, columnar workloads and integrates seamlessly with Python, dbt, and Metabase. DuckDB enables fast exploration of aggregated sensor and lab data without standing up a full-scale database server, making it ideal for edge deployments and prototype scenarios.</p>"},{"location":"RPi5/#postgresql","title":"PostgreSQL","text":"<p>Serves as the OLTP data store for sensor readings, reference tables, and structured logs. Its schema follows the medallion architecture (bronze and silver), enabling clear data lineage and progressive refinement.</p>"},{"location":"RPi5/#postgis","title":"PostGIS","text":"<p>An extension to PostgreSQL that enables advanced geospatial data support. PostGIS allows the stack to handle spatial queries, store geometries (points, lines, polygons), and perform GIS-style analytics. It is ideal for modeling field assets, service areas, and location-based events directly within the relational database. Let me </p>"},{"location":"RPi5/#node-exporter","title":"Node Exporter","text":"<p>Lightweight exporter that exposes live Linux system metrics (CPU, RAM, disk, network) on port <code>:9100</code>.</p>"},{"location":"RPi5/#prometheus","title":"Prometheus","text":"<p>Pulls metrics from Node Exporter on a scheduled interval. Stores time-series data used for dashboard visualizations and potential alerting.</p>"},{"location":"RPi5/#grafana","title":"Grafana","text":"<p>Provides the visualization layer for operational metrics. Dashboards track system health and performance using Prometheus data. Layouts and configurations are stored in Git.</p>"},{"location":"RPi5/#metabase","title":"Metabase","text":"<p>Acts as a lightweight BI layer over PostgreSQL\u2019s silver tables. Offers drag-and-drop charting, filtering, and dashboarding capabilities without requiring code\u2014ideal for OLAP exploration.</p>"},{"location":"RPi5/#dbt-data-build-tool","title":"dbt (data build tool)","text":"<p>Manages SQL-based data transformations, converting bronze data into tested, normalized silver views. All transformation models are stored in <code>~/scr/06_dbt</code>.</p>"},{"location":"RPi5/#dagster","title":"Dagster","text":"<p>Handles data orchestration, asset scheduling, and pipeline observability. Operates via a local webserver on port <code>3300</code> and executes ETL pipelines from CSV to PostgreSQL using Python-defined assets.</p>"},{"location":"RPi5/#mkdocs","title":"MkDocs","text":"<p>Provides the governance layer by serving structured project documentation aligned to DMBOK. It supports: - Linking technical assets to business glossary terms - Surfacing metadata (like last updated, version control) - Improving transparency and knowledge transfer MkDocs bridges the gap between engineering outputs and business context.</p>"},{"location":"RPi5/#nginx","title":"NGINX","text":"<p>Provides a secure reverse proxy for accessing internal services (Grafana, Metabase, Dagster). Supports TLS encryption and optional authentication to simulate real-world access patterns and security posture.</p>"},{"location":"ScrumKanban/","title":"Agile Scrum and Kanban: Companion Guide for Delivery Teams","text":"<p>This guide introduces Scrum and Kanban, two Agile delivery frameworks used to manage iterative work, increase visibility, and respond to change. Designed for engineering, IT, GIS, field services, and utility project teams, these methods support structured execution and continuous improvement.</p>"},{"location":"ScrumKanban/#agile-scrum-vs-kanban-at-a-glance","title":"Agile Scrum vs Kanban at a Glance","text":"Feature Scrum Kanban Delivery rhythm Fixed sprints (2\u20134 weeks) Continuous flow Roles Scrum Master, Product Owner, Team Optional (can use facilitator) Planning Sprint planning per cycle Pull-based with limits Commitments Sprint goal and backlog WIP limits, flow optimization Ideal for Feature delivery, cross-functional teams Reactive ops, BAU, support Meetings Daily standups, reviews, retrospectives Optional but recommended Tracking Velocity and burndown charts Lead time and cumulative flow"},{"location":"ScrumKanban/#scrum-the-iterative-delivery-framework","title":"Scrum: The Iterative Delivery Framework","text":""},{"location":"ScrumKanban/#scrum-roles","title":"Scrum Roles","text":"<ul> <li>Product Owner: Prioritizes the backlog and represents stakeholder needs</li> <li>Scrum Master: Facilitates the process, removes blockers, and supports the team</li> <li>Team Members: Deliver potentially shippable increments of work each sprint</li> </ul>"},{"location":"ScrumKanban/#scrum-events","title":"Scrum Events","text":"Event Description Sprint Planning Define the sprint goal and select backlog items Daily Standup Short check-in to share progress, blockers, and plans Sprint Review Demonstrate completed work to stakeholders Sprint Retrospective Team reflects on process and suggests improvements"},{"location":"ScrumKanban/#scrum-artifacts","title":"Scrum Artifacts","text":"<ul> <li>Product Backlog: Ordered list of all features, tasks, bugs, and tech debt</li> <li>Sprint Backlog: Items committed to during the sprint</li> <li>Increment: Completed, potentially shippable work output</li> </ul>"},{"location":"ScrumKanban/#kanban-the-visual-flow-framework","title":"Kanban: The Visual Flow Framework","text":"<p>Kanban is a pull-based, visual system for managing work in progress (WIP). It helps teams improve flow efficiency and limit overcommitment.</p>"},{"location":"ScrumKanban/#core-concepts","title":"Core Concepts","text":"<ul> <li>Visual Board: Tasks move through columns (e.g., To Do \u2192 Doing \u2192 Done)</li> <li>Work-in-Progress (WIP) Limits: Caps the number of active tasks to improve focus</li> <li>Cycle Time: Measures how long it takes for a task to move from start to finish</li> <li>Pull System: Team members \u201cpull\u201d new work when capacity allows</li> </ul>"},{"location":"ScrumKanban/#typical-kanban-workflow","title":"Typical Kanban Workflow","text":"<p>[ Backlog ] \u2192 [ Ready ] \u2192 [ In Progress ] \u2192 [ Review ] \u2192 [ Done ]</p>"},{"location":"ScrumKanban/#when-to-use-scrum-vs-kanban","title":"When to Use Scrum vs Kanban","text":"Use Scrum When... Use Kanban When... Work can be broken into planned iterations Work arrives unpredictably (e.g., service requests) Team delivers features or product increments Team handles BAU, bug fixes, field tickets Stakeholders need regular demos or goals Team values minimizing cycle time and WIP You're building something new You're optimizing workflows"},{"location":"ScrumKanban/#agile-board-examples","title":"Agile Board Examples","text":""},{"location":"ScrumKanban/#scrum-example-2-week-sprint","title":"Scrum Example (2-week sprint)","text":"Status Tasks To Do Build lab data connector, mock KPI report In Progress Create WIMS-to-Postgres ETL job Review SCADA API authentication logic Done MVP dashboard wireframe"},{"location":"ScrumKanban/#kanban-example-ongoing","title":"Kanban Example (Ongoing)","text":"Status Tasks Backlog Add meter dataset to Grafana Ready Fix mobile form GPS sync bug In Progress Migrate schema to v2 Review Update SOP documentation Done Archive 2023 water quality data"},{"location":"ScrumKanban/#metrics-and-continuous-improvement","title":"Metrics and Continuous Improvement","text":"Metric Scrum Use Kanban Use Description Velocity \u2714\ufe0f \u274c Total story points completed per sprint Lead Time \u274c \u2714\ufe0f Time from task creation to completion Cycle Time \u274c \u2714\ufe0f Time from start of work to completion Burndown \u2714\ufe0f \u274c Remaining work vs time in sprint Cumulative Flow \u274c \u2714\ufe0f Visualizes bottlenecks across statuses"},{"location":"ScrumKanban/#success-factors","title":"Success Factors","text":"<ul> <li>Keep boards visible and updated (e.g., Jira, DevOps, Trello)</li> <li>Review progress regularly with stakeholders</li> <li>Reflect and improve\u2014don\u2019t skip retrospectives or process reviews</li> <li>Limit WIP to reduce multitasking and increase throughput</li> <li>Focus on outcomes, not just velocity or story count</li> </ul>"},{"location":"ScrumKanban/#further-reading","title":"Further Reading","text":"<ul> <li>https://scrumguides.org</li> <li>https://kanbanguides.org</li> <li>https://www.atlassian.com/agile/tutorials</li> <li>https://learn.microsoft.com/en-us/azure/devops/boards/plans/</li> <li>https://18f.gsa.gov</li> </ul>"},{"location":"ScrumKanban/#template-lightweight-scrum-sprint-planning","title":"Template: Lightweight Scrum Sprint Planning","text":"<p>```markdown</p>"},{"location":"ScrumKanban/#sprint-goal","title":"Sprint Goal","text":"<p>Enable real-time alerting for water quality exceedances</p>"},{"location":"ScrumKanban/#planned-stories","title":"Planned Stories","text":"<ul> <li> Build alert engine using threshold logic</li> <li> Connect alert engine to SCADA stream</li> <li> Notify on-call staff via Teams integration</li> <li> Write SOP for triage workflow</li> </ul>"},{"location":"ScrumKanban/#sprint-duration","title":"Sprint Duration","text":"<p>March 1 \u2013 March 14</p>"},{"location":"ScrumKanban/#definition-of-done","title":"Definition of Done","text":"<ul> <li>All acceptance tests pass</li> <li>Deployed to staging</li> <li>Stakeholder approval demo complete</li> </ul>"},{"location":"Scrumban/","title":"Scrumban Agile Framework with OKR + Sprint Tracker Template","text":"<p>A practical hybrid approach combining Scrum structure and Kanban flow\u2014tailored for delivery teams in infrastructure, utilities, IT, GIS, and digital modernization.</p>"},{"location":"Scrumban/#when-to-use-scrumban","title":"\ud83e\udded When to Use Scrumban","text":"<p>Use Scrumban if: - Your team handles both feature development and operational support. - You want iterative planning without committing to strict sprint cadences. - You need visibility and flow management for ongoing or reactive work. - You're modernizing from traditional waterfall processes.</p>"},{"location":"Scrumban/#key-characteristics","title":"\ud83d\udd27 Key Characteristics","text":"Element Description Planning Cadence Optional; use weekly/biweekly syncs or just-in-time planning Work-in-Progress Limits Limit active work per status column to maintain flow Pull-Based System Team members pull work based on capacity Backlog Grooming Continuous prioritization instead of big batch sprint planning Standups Short daily or twice-weekly check-ins Retrospectives Regular (every 2\u20134 weeks) process reflection Tagging/Swimlanes Label work by theme: OKRs, Support, Compliance, Tech Debt"},{"location":"Scrumban/#example-scrumban-board","title":"\ud83c\udfa8 Example Scrumban Board","text":"<p>[ Backlog ] \u2192 [ Ready for Development ] \u2192 [ In Progress ] (WIP: 3) \u2192 [ In Review ] \u2192 [ Blocked ] \u2192 [ Done ]</p> <p>Use swimlanes or tags such as #OKR, #Ops, #Fieldwork, #GIS, #Support.</p>"},{"location":"Scrumban/#example-task-card","title":"\ud83d\udcdd Example Task Card","text":""},{"location":"Scrumban/#task-integrate-turbidity-sensors-into-scada-dashboard","title":"Task: Integrate turbidity sensors into SCADA dashboard","text":"<ul> <li>Tags: Feature, SCADA, OKR-Q2</li> <li>Status: In Progress</li> <li>WIP Limit: 3 items</li> <li>Related Objective: Enhance visibility into water quality conditions</li> </ul>"},{"location":"Scrumban/#okr-sprint-tracker-template","title":"\ud83d\udccb OKR + Sprint Tracker Template","text":""},{"location":"Scrumban/#objective","title":"Objective","text":"<p>Improve transparency and decision support with real-time operations dashboards</p>"},{"location":"Scrumban/#key-results","title":"Key Results","text":"<ul> <li> Deploy MVP dashboard with live data by May 15  </li> <li> Complete stakeholder validation with 90% satisfaction score  </li> <li> Train 20 users on filtering and reporting capabilities  </li> <li> Reduce average report generation time from 48h to &lt;8h  </li> </ul>"},{"location":"Scrumban/#sprint-tracker-q2-sprint-2-may-1-may-15","title":"Sprint Tracker \u2014 Q2 Sprint 2 (May 1 \u2013 May 15)","text":""},{"location":"Scrumban/#sprint-goal","title":"Sprint Goal","text":"<p>Deliver first end-to-end functional version of dashboard with water quality indicators.</p>"},{"location":"Scrumban/#in-progress","title":"In Progress","text":"<ul> <li> Develop PostgreSQL view for SCADA stream ingest  </li> <li> Build dashboard layout in Power BI with drilldowns  </li> <li> Configure role-based access for viewer/editor modes  </li> </ul>"},{"location":"Scrumban/#completed","title":"Completed","text":"<ul> <li> Wireframe and data mapping validated  </li> <li> Data refresh frequency set to hourly  </li> </ul>"},{"location":"Scrumban/#blockers","title":"Blockers","text":"<ul> <li>Pending firewall exception for SCADA-to-staging DB route  </li> <li>Clarification needed on unit conversion rules from WIMS  </li> </ul>"},{"location":"Scrumban/#velocity-notes","title":"Velocity Notes","text":"<ul> <li>Story points completed: 18 (target: 20)  </li> <li>Key Result progress: 60% overall  </li> <li>Team notes: Good velocity, need clearer acceptance criteria for KR2  </li> </ul>"},{"location":"Scrumban/#retrospective-notes","title":"Retrospective Notes","text":"<ul> <li>\u2705 What went well: Early testing of live refresh improved UI decisions  </li> <li>\u26a0\ufe0f What to improve: Better coordination on IT security review timelines  </li> <li>\ud83d\udccc Action items: Create SCADA access checklist for future teams  </li> </ul>"},{"location":"Scrumban/#tips-for-adopting-scrumban","title":"\ud83e\udde0 Tips for Adopting Scrumban","text":"<ul> <li>Start with your current board and add WIP limits gradually.  </li> <li>Encourage tagging OKR-aligned work to keep strategic priorities visible.  </li> <li>Make retrospectives short, regular, and focused on process\u2014not blame.  </li> <li>Use dashboards (Power BI, Grafana, Confluence) to make progress visible.  </li> <li>Don\u2019t wait for perfection\u2014improve one small thing every cycle.</li> </ul>"},{"location":"Scrumban/#references","title":"\ud83d\udcda References","text":"<ul> <li>https://www.atlassian.com/agile/scrumban  </li> <li>https://scrumguides.org  </li> <li>https://kanbanguides.org  </li> <li>https://www.whatmatters.com  </li> <li>https://learn.microsoft.com/en-us/azure/devops/boards/plans/  </li> </ul>"},{"location":"SkillLevels/","title":"Data Engineer Advanced Skills","text":"<p>A comprehensive, tagged guide with concept explanations for developing advanced skills in data engineering.</p>"},{"location":"SkillLevels/#1-cs-fundamentals","title":"1. CS Fundamentals","text":""},{"location":"SkillLevels/#basic-terminal-usage","title":"Basic terminal usage","text":"<p>Use command-line interfaces (CLI) to navigate, run scripts, manage files, and control environments. Concept: The CLI allows powerful automation and scripting. Knowing commands like <code>ls</code>, <code>cd</code>, <code>grep</code>, and piping (<code>|</code>) unlocks Unix systems.</p>"},{"location":"SkillLevels/#data-structures-algorithms","title":"Data structures &amp; algorithms","text":"<p>Understand how data is organized and how to manipulate it efficiently. Concept: Learn arrays, lists, dictionaries (hash maps), trees, and graphs. Algorithmic thinking improves performance and reduces bottlenecks in ETL pipelines.</p>"},{"location":"SkillLevels/#apis","title":"APIs","text":"<p>Application Programming Interfaces define how programs interact. Concept: Understand how to call and parse API responses (often JSON). REST and GraphQL are dominant formats for accessing external or internal services.</p>"},{"location":"SkillLevels/#rest","title":"REST","text":"<p>A widely used web architecture style for APIs based on stateless calls. Concept: Learn HTTP methods (GET, POST, PUT, DELETE), status codes, and endpoints. Critical for integrating third-party and internal services.</p>"},{"location":"SkillLevels/#structured-vs-unstructured-data","title":"Structured vs unstructured data","text":"<p>Differentiate data with fixed schemas (tables) vs. free-form content (logs, PDFs). Concept: ETL strategies differ based on data structure. Use schema-on-read tools (e.g., Spark) for unstructured data.</p>"},{"location":"SkillLevels/#serialisation","title":"Serialisation","text":"<p>Convert data structures into a format for storage or transmission (e.g., JSON, Parquet). Concept: Learn formats like Avro, JSON, Protobuf, and Parquet. Understand trade-offs: human-readable vs efficient encoding.</p>"},{"location":"SkillLevels/#linux","title":"Linux","text":"<p>Most data platforms run on Linux; know basic ops and scripting. - CLI: Work within the shell (bash/zsh) to manage files and processes. - Vim: Text editor commonly found on all Linux systems. - Shell scripting: Automate workflows using <code>.sh</code> files. - Cronjobs: Schedule recurring jobs using cron. Concept: Become fluent in daily terminal tasks; it's essential for debugging and automation.</p>"},{"location":"SkillLevels/#how-does-the-computer-work","title":"How does the computer work?","text":"<p>Understand basic hardware, CPU, memory, and file I/O. Concept: Helps when tuning performance and debugging pipeline or storage latency.</p>"},{"location":"SkillLevels/#how-does-the-internet-work","title":"How does the Internet work?","text":"<p>Understand DNS, HTTP, TCP/IP, and routing. Concept: Critical for debugging data ingestion from external APIs or services.</p>"},{"location":"SkillLevels/#git-version-control","title":"Git \u2014 Version control","text":"<p>Track changes, manage branches, and collaborate on code. Concept: Learn <code>git clone</code>, <code>commit</code>, <code>push</code>, <code>merge</code>, and <code>rebase</code>. Git is also used in data CI/CD workflows (e.g., dbt, Airflow DAGs).</p>"},{"location":"SkillLevels/#math-statistics-basics","title":"Math &amp; statistics basics","text":"<p>Understand probability, distributions, averages, and basic inference. Concept: Useful for quality checks, anomaly detection, and working with ML teams.</p>"},{"location":"SkillLevels/#2-programming-languages","title":"2. Programming Languages","text":""},{"location":"SkillLevels/#python","title":"Python","text":"<p>A versatile language for scripting, ETL, APIs, dataframes, and orchestration. Concept: Know <code>pandas</code>, <code>sqlalchemy</code>, <code>requests</code>, and multiprocessing. Python dominates the modern DE stack.</p>"},{"location":"SkillLevels/#java","title":"Java","text":"<p>Often used in enterprise tools (e.g., Hadoop, Kafka, Beam). Concept: Required to extend or debug JVM-based big data tools.</p>"},{"location":"SkillLevels/#scala","title":"Scala","text":"<p>Functional programming language for Spark and streaming workloads. Concept: Learn immutability, map/reduce operations, and Spark transformations.</p>"},{"location":"SkillLevels/#go","title":"Go","text":"<p>Compiled language for building fast, concurrent systems. Concept: Some modern DE tools (e.g., Prometheus, Loki) use Go; useful for writing high-performance services.</p>"},{"location":"SkillLevels/#3-testing","title":"3. Testing","text":""},{"location":"SkillLevels/#unit-testing","title":"Unit testing","text":"<p>Validate individual components of a script or module. Concept: Learn <code>pytest</code> or <code>unittest</code> in Python. Test each function with mocks and assertions.</p>"},{"location":"SkillLevels/#integration-testing","title":"Integration testing","text":"<p>Test how multiple components work together. Concept: Useful when testing Airflow DAGs, database connections, and API endpoints.</p>"},{"location":"SkillLevels/#functional-testing","title":"Functional testing","text":"<p>Verify that the system performs as expected from an end-user perspective. Concept: Applies to entire pipeline runs or API inputs/outputs.</p>"},{"location":"SkillLevels/#property-based-testing","title":"Property-based testing","text":"<p>Test functions with a wide range of randomized inputs. Concept: Use tools like Hypothesis to ensure robustness of data transformations.</p>"},{"location":"SkillLevels/#4-database-fundamentals","title":"4. Database Fundamentals","text":""},{"location":"SkillLevels/#sql","title":"SQL","text":"<p>The standard language for querying and manipulating structured data in relational databases. Concept: Learn <code>SELECT</code>, <code>JOIN</code>, <code>GROUP BY</code>, subqueries, window functions, and CTEs.</p>"},{"location":"SkillLevels/#normalisation","title":"Normalisation","text":"<p>Organize data to eliminate redundancy and improve data integrity. Concept: Understand 1NF to 3NF and how denormalization can be used for analytics.</p>"},{"location":"SkillLevels/#acid-transactions","title":"ACID transactions","text":"<p>Ensure that database operations are Atomic, Consistent, Isolated, and Durable. Concept: Guarantees reliability in transactional systems, crucial for OLTP.</p>"},{"location":"SkillLevels/#cap-theorem","title":"CAP theorem","text":"<p>Describes trade-offs between Consistency, Availability, and Partition Tolerance in distributed systems. Concept: You can only have two out of three \u2014 know what your database prioritizes.</p>"},{"location":"SkillLevels/#oltp-vs-olap","title":"OLTP vs OLAP","text":"<p>Distinguish between systems optimized for transactional processing vs analytical querying. Concept: OLTP (e.g., PostgreSQL) is row-oriented and normalized; OLAP (e.g., Redshift) is columnar and denormalized.</p>"},{"location":"SkillLevels/#horizontal-vs-vertical-scaling","title":"Horizontal vs vertical scaling","text":"<p>Ways to increase capacity by adding machines or resources. Concept: Horizontal scaling (sharding/distribution); vertical scaling (larger server with more RAM/CPU).</p>"},{"location":"SkillLevels/#dimensional-modeling","title":"Dimensional modeling","text":"<p>Design schemas that optimize analytics performance. Concept: Use star or snowflake schemas with fact and dimension tables.</p>"},{"location":"SkillLevels/#indexing-strategies","title":"Indexing strategies","text":"<p>Improve query performance by organizing data for fast access. Concept: Learn B-tree, bitmap, and hash indexes; understand when to index and how to interpret query plans.</p>"},{"location":"SkillLevels/#query-optimization","title":"Query optimization","text":"<p>Tuning queries for speed and resource efficiency. Concept: Use <code>EXPLAIN ANALYZE</code>, reduce nested queries, avoid Cartesian products, and leverage indexes.</p> <p>... [Truncated for brevity in this sample output. Would you like the full document in a downloadable or full-page format? I can continue with the rest if you confirm.]</p>"},{"location":"StackComparisons/","title":"Modern Analytics Stack Comparisons","text":""},{"location":"StackComparisons/#a-platform-agnostic-view-of-the-data-ecosystem","title":"A Platform-Agnostic View of the Data Ecosystem","text":"<p>This document compares the core components of modern analytics stacks across four major ecosystems:</p> <ul> <li>On-Prem / Open Source</li> <li>Amazon Web Services (AWS)</li> <li>Google Cloud Platform (GCP)</li> <li>Microsoft Azure</li> </ul> <p>Each layer highlights tools for data ingestion, processing, storage, transformation, orchestration, governance, observability, and visualization.</p>"},{"location":"StackComparisons/#storage-data-layers","title":"Storage &amp; Data Layers","text":"Category On-Prem / Open Source AWS GCP Azure Data Lake Storage HDFS, MinIO, Apache Ozone Amazon S3 Google Cloud Storage Azure Data Lake Storage Gen2 Object Storage Ceph, MinIO Amazon S3 GCS Azure Blob Storage Data Warehouse / OLAP ClickHouse, DuckDB, Druid Redshift, Athena BigQuery Synapse Analytics Lakehouse Query Layer Trino, Presto, Dremio Athena, Redshift Spectrum BigQuery (federated), Dremio Synapse Serverless, ADX Relational DB (OLTP) PostgreSQL, MySQL, MariaDB RDS, Aurora Cloud SQL, AlloyDB Azure SQL DB, PostgreSQL"},{"location":"StackComparisons/#ingestion-data-movement","title":"Ingestion &amp; Data Movement","text":"Category On-Prem / Open Source AWS GCP Azure Stream Ingestion Apache Kafka, Redpanda Kinesis Data Streams Pub/Sub Azure Event Hubs Batch Ingestion / ETL Airbyte, Nifi, Singer AWS Glue, Data Pipeline Cloud Dataflow, Composer Data Factory, Synapse Pipelines Streaming Processing Apache Flink, Kafka Streams Kinesis Analytics, MSK+Flink Dataflow (Apache Beam) Azure Stream Analytics"},{"location":"StackComparisons/#orchestration-transformation","title":"Orchestration &amp; Transformation","text":"Category On-Prem / Open Source AWS GCP Azure Orchestration Airflow, Dagster MWAA, Step Functions Composer (Managed Airflow) Data Factory, Synapse Pipelines Data Transformation (ELT) dbt-core dbt Cloud on ECS, Glue, Lambda dbt Cloud on BigQuery dbt Cloud on Synapse or SQL DB"},{"location":"StackComparisons/#metadata-lineage-governance","title":"Metadata, Lineage &amp; Governance","text":"Category On-Prem / Open Source AWS GCP Azure Metadata Catalog Amundsen, DataHub, OpenMetadata Glue Data Catalog Google Data Catalog Microsoft Purview Data Lineage Marquez, OpenLineage Glue + OpenLineage Catalog + Lineage Preview Purview + Lineage API Access Control OPA, Keycloak IAM, Lake Formation IAM, VPC-SC Azure AD, RBAC Governance Tools Apache Ranger, Vault, OPA Lake Formation, Macie Dataplex, DLP Purview, Defender for Cloud"},{"location":"StackComparisons/#visualization-bi","title":"Visualization &amp; BI","text":"Category On-Prem / Open Source AWS GCP Azure BI Tools Superset, Metabase, Redash QuickSight Looker, Data Studio Power BI"},{"location":"StackComparisons/#ml-ai-integration","title":"ML &amp; AI Integration","text":"Category On-Prem / Open Source AWS GCP Azure ML Integration MLflow, Kubeflow, JupyterHub SageMaker, EMR Notebooks Vertex AI, Colab, AI Platform Azure ML, Synapse ML"},{"location":"StackComparisons/#observability-data-quality","title":"Observability &amp; Data Quality","text":"Category Example Tools Data Validation Great Expectations, dbt tests, Soda SQL Pipeline Monitoring Airflow UI, Dagster UI, Prefect Cloud Data Drift &amp; Anomalies Monte Carlo, Databand, Bigeye, OpenMetadata alerts Logging &amp; Metrics Prometheus, ELK Stack, CloudWatch, Azure Monitor"},{"location":"StackComparisons/#common-use-case-mappings","title":"Common Use Case Mappings","text":"Use Case Example Stack Lightweight, portable analytics DuckDB + dbt-core + Superset Serverless analytics with ML BigQuery + Vertex AI + Looker High-governance enterprise BI Synapse + Purview + Power BI Real-time analytics with pipelines Kafka + Flink + ClickHouse or Redshift Data mesh with semantic governance dbt Cloud + OpenMetadata + domain-specific BI"},{"location":"StackComparisons/#references","title":"References","text":"<ul> <li>https://www.getdbt.com \u2013 dbt transformation framework  </li> <li>https://openmetadata.io \u2013 Open-source metadata and governance  </li> <li>https://aws.amazon.com/lake-formation/ \u2013 AWS governance &amp; security  </li> <li>https://cloud.google.com/dataplex \u2013 GCP metadata and data mesh  </li> <li>https://azure.microsoft.com/en-us/services/purview/ \u2013 Microsoft Purview governance platform</li> </ul>"},{"location":"TieredAnalytics/","title":"From Raw Data to Reliable Insights: A Tiered Approach for Water Utilities","text":"<p>Water utilities rely on a growing stream of data \u2014 from SCADA, CMMS, and LIMS systems to field reports and lab results. To transform this data into insights, it must be structured, cleaned, and contextualized so that engineers, analysts, and executives can all trust what they\u2019re seeing.</p> <p>One proven way to do this is by following a layered data architecture \u2014 a method popularized by data warehousing pioneer Ralph Kimball, and today adapted by both open-source and cloud-native platforms. Kimball emphasized clarity, usability, and dimensional modeling \u2014 shaping data to reflect the way people think and make decisions.</p> <p>This approach is commonly referred to as the Bronze, Silver, and Gold model, especially in platforms like Databricks. But the same pattern appears across tools with different labels:</p> <ul> <li>Bronze is also known as Raw, Staging, or Landing Zone</li> <li>Silver is referred to as Cleaned, Core, Conformed, or Integrated</li> <li>Gold goes by Semantic Layer, Presentation, Mart, or Business KPIs</li> </ul> <p>These tiers also align with how data systems behave technically: - The Bronze layer typically uses OLTP systems (Online Transaction Processing) \u2014 optimized for high-speed data entry, retrieval, and row-level accuracy - The Gold layer relies on OLAP systems (Online Analytical Processing) \u2014 built for slicing, aggregating, and drilling into data for reporting and dashboards - The Silver layer sits in between \u2014 using OLTP-style storage with increasing OLAP behaviors as data is joined, transformed, and analyzed</p> <p>This layered model creates structure and accountability \u2014 so analysts can explore freely, and decision-makers can trust the numbers.</p>"},{"location":"TieredAnalytics/#bronze-raw-data-and-the-source-of-truth","title":"\ud83d\udfeb Bronze \u2013 Raw Data and the Source of Truth","text":"<p>Bronze is the raw ingestion layer. It captures every file, reading, and log exactly as it was received \u2014 from SCADA sensors, CMMS exports, LIMS reports, spreadsheets, and APIs.</p> <p>Nothing is filtered or modified. Every record is timestamped and stored for traceability. This layer serves as the source of truth \u2014 critical for audits, data science, historical investigations, or future reprocessing if definitions change.</p> <p>Technically, Bronze often lives in OLTP systems like PostgreSQL \u2014 designed for quick, accurate capture of row-level data.</p> <p>Used by: SCADA engineers, developers, data scientists building models or monitoring ML drift, compliance and IT teams</p> <p>Purpose: Preserve full-fidelity source data; support audits, modeling, and traceability</p> <p>Also known as: Raw, Staging, Landing, Base Layer</p>"},{"location":"TieredAnalytics/#silver-clean-data-for-exploration-and-innovation","title":"\ud83e\ude99 Silver \u2013 Clean Data for Exploration and Innovation","text":"<p>Silver is the refined and integrated layer. Here, raw records are cleaned, deduplicated, and aligned across systems \u2014 such as linking pump runtimes with work orders, or joining lab results with field inspections.</p> <p>This layer powers ad hoc analysis, dashboards, and exploratory insight. Analysts and engineers use Silver to create custom filters, calculate new metrics, and answer operational questions \u2014 often through tools like Power BI or Python notebooks.</p> <p>While still stored in OLTP-like systems, Silver begins to show OLAP characteristics: dimensional joins, aggregations, and slicing across time, site, and asset class.</p> <p>Used by: Analysts, engineers, power users, dashboard authors, planning teams</p> <p>Purpose: Support diagnostics, self-service analysis, metric development, and operational reporting</p> <p>Also known as: Cleaned, Integrated, Core, Conformed, Intermediate Layer</p>"},{"location":"TieredAnalytics/#gold-trusted-metrics-for-decision-making","title":"\ud83e\udd47 Gold \u2013 Trusted Metrics for Decision-Making","text":"<p>Gold is the semantic layer \u2014 a curated, documented, and versioned set of business metrics. It defines KPIs like \"chlorine compliance rate,\" \"average days to close a work order,\" or \"downtime per pump.\"</p> <p>Gold is where enterprise alignment happens. Metrics here are governed and consistent \u2014 used across executive dashboards, finance reports, board briefings, and regulatory submissions.</p> <p>These datasets typically live in OLAP-optimized systems \u2014 such as DuckDB or BI models in Power BI \u2014 supporting fast reads, aggregations, and secure slicing by geography or time.</p> <p>Used by: Directors, general managers, compliance leads, finance and strategy teams</p> <p>Purpose: Power strategic dashboards, ensure metric consistency, drive regulatory and performance reporting</p> <p>Also known as: Presentation Layer, KPI Model, Semantic Layer, Business Mart</p>"},{"location":"TieredAnalytics/#tier-based-update-intervals","title":"\ud83d\udd01 Tier-Based Update Intervals","text":"<p>Each layer in this architecture has its own ideal update frequency \u2014 based on its users, purpose, and technical requirements.</p> <p>Bronze \u2013 Frequent or Real-Time This layer ingests raw data as it arrives. SCADA readings, sensor logs, and CSV exports should be written to Bronze as frequently as needed \u2014 often every few minutes or hourly \u2014 to preserve completeness and support real-time models.</p> <p>Silver \u2013 Hourly to Daily Silver should be updated thoughtfully. It serves users exploring trends or diagnosing recent events. Updates every hour or once daily are typically sufficient. This timing avoids propagating unvalidated data and gives time for QA processes to catch issues.</p> <p>Gold \u2013 Daily to Weekly Gold values consistency over freshness. Daily or weekly updates ensure stable KPIs for reporting and decision-making. Changes should follow a versioned workflow, with clear documentation and review.</p> <p>Rule of thumb:  </p> <p>Fastest updates in Bronze, careful cadence in Silver, and deliberate governance in Gold.</p>"},{"location":"TieredAnalytics/#hybrid-governance-encouraging-innovation-without-fragmentation","title":"\ud83e\udd1d Hybrid Governance: Encouraging Innovation Without Fragmentation","text":"<p>To stay agile, teams must have the freedom to experiment \u2014 but also the structure to ensure shared truth. In a hybrid governance model, each tier plays a role in both exploration and standardization:</p> <ol> <li> <p>Explore in Silver    Analysts and power users can prototype new metrics and filters using cleaned, reliable Silver-layer data. These may include new classifications, time windows, or operational definitions relevant to a specific site or department.</p> </li> <li> <p>Model from Bronze    Data scientists and advanced users can work directly with raw Bronze data to build machine learning models, conduct anomaly detection, or generate new operational insights. Starting from the unfiltered source ensures reproducibility, full context, and access to outliers or edge cases that might be cleaned away in Silver.</p> </li> <li> <p>Promote to Gold    When a metric, model output, or derived feature proves valuable:</p> </li> <li>It is reviewed by data leads and operational SMEs</li> <li>The logic or model output is formalized in DBT or SQL</li> <li>It is added to the governed Gold layer for broad reuse</li> <li> <p>Dashboards and reports are updated to reflect the shared definition</p> </li> <li> <p>Track lineage    Maintain a register of all metrics and model-derived outputs that graduate from Silver or Bronze to Gold. This helps promote accountability, document assumptions, and create a feedback loop for future refinement.</p> </li> <li> <p>Protect enterprise outputs    Dashboards used for strategic planning, board briefings, or regulatory submissions should only draw from the Gold layer to ensure stability, explainability, and data trustworthiness.</p> </li> </ol> <p>This hybrid approach allows analysts and modelers to innovate, while protecting business users from conflicting definitions or unverified results. It also ensures that AI and machine learning outputs contribute to \u2014 rather than compete with \u2014 the organization's shared understanding of performance.</p>"},{"location":"TieredAnalytics/#why-this-works-for-water-utilities","title":"\u2705 Why This Works for Water Utilities","text":"<p>Water agencies don\u2019t need a massive tech stack \u2014 they need structure, transparency, and alignment.</p> <ul> <li>Bronze ensures nothing is lost or overwritten</li> <li>Silver empowers analysts to explore operational trends</li> <li>Gold creates consistency and clarity in metrics that matter</li> </ul>"},{"location":"UtilityTheory/","title":"Ralph Keeney's Utility Theory","text":"<p>Ralph Keeney is a foundational figure in decision analysis, known especially for his development and application of utility theory and multiattribute utility theory (MAUT). His contributions provide a systematic way to support rational decision-making under uncertainty by quantifying preferences and tradeoffs across multiple, often conflicting, objectives.</p>"},{"location":"UtilityTheory/#1-introduction-to-utility-theory","title":"1. Introduction to Utility Theory","text":"<p>Utility theory offers a mathematical framework to model how people make decisions when faced with uncertainty and multiple conflicting goals. It defines a utility function to quantify the desirability of different outcomes based on an individual's or organization\u2019s preferences.</p> <p>The central idea is that a decision maker should choose the alternative with the highest expected utility, not just the highest probability or lowest cost.</p>"},{"location":"UtilityTheory/#2-single-attribute-vs-multiattribute-utility","title":"2. Single-Attribute vs Multiattribute Utility","text":""},{"location":"UtilityTheory/#single-attribute-utility","title":"Single-Attribute Utility","text":"<p>When evaluating options based on a single factor (e.g., water quality score, system uptime), a single-attribute utility function assigns a number to each outcome representing its value or desirability.</p> <p>Example: - Utility of water quality index (0 to 1) - Utility of cost (inverted so lower cost has higher utility)</p>"},{"location":"UtilityTheory/#multiattribute-utility-mau","title":"Multiattribute Utility (MAU)","text":"<p>Keeney extended utility theory to situations involving multiple criteria. In MAUT, overall utility is calculated by combining the utility of each attribute, weighted by its importance.</p> <p>Additive MAUT Formula: <pre><code>U(x) = w\u2081\u00b7u\u2081(x\u2081) + w\u2082\u00b7u\u2082(x\u2082) + ... + w\u2099\u00b7u\u2099(x\u2099)\n</code></pre> - <code>U(x)</code> = overall utility of outcome <code>x</code> - <code>x\u1d62</code> = value of attribute <code>i</code> - <code>u\u1d62(x\u1d62)</code> = utility function for attribute <code>i</code> - <code>w\u1d62</code> = weight of importance for attribute <code>i</code> (sum of all weights = 1)</p> <p>Keeney\u2019s work ensures logical consistency in how tradeoffs are evaluated between different decision criteria (e.g., cost vs. reliability vs. environmental impact).</p>"},{"location":"UtilityTheory/#3-value-focused-thinking","title":"3. Value-Focused Thinking","text":"<p>Rather than starting with a list of alternatives, Keeney proposed Value-Focused Thinking (VFT). This approach emphasizes first identifying values (i.e., what matters) before generating and comparing options.</p>"},{"location":"UtilityTheory/#core-vft-process","title":"Core VFT Process","text":""},{"location":"UtilityTheory/#1-define-objectives","title":"1. Define Objectives","text":"<ul> <li>What are we trying to achieve?</li> <li>Example: Improve resilience, minimize lifecycle costs, ensure regulatory compliance</li> </ul>"},{"location":"UtilityTheory/#2-develop-value-measures","title":"2. Develop Value Measures","text":"<ul> <li>Create measurable indicators for each objective</li> <li>Example: Uptime %, cost per connection, risk index</li> </ul>"},{"location":"UtilityTheory/#3-structure-the-utility-function","title":"3. Structure the Utility Function","text":"<ul> <li>Use stakeholder input to assign weights and normalize utility functions</li> </ul>"},{"location":"UtilityTheory/#4-generate-alternatives","title":"4. Generate Alternatives","text":"<ul> <li>Innovate new solutions using objectives as guidance</li> </ul>"},{"location":"UtilityTheory/#5-evaluate-alternatives","title":"5. Evaluate Alternatives","text":"<ul> <li>Use the utility model to score and compare alternatives</li> </ul>"},{"location":"UtilityTheory/#quote-from-keeney","title":"Quote from Keeney","text":"<p>\"A decision problem is a clarification of values, not just a selection among alternatives.\"</p>"},{"location":"UtilityTheory/#4-applications-of-keeneys-utility-theory","title":"4. Applications of Keeney\u2019s Utility Theory","text":""},{"location":"UtilityTheory/#infrastructure-planning","title":"Infrastructure Planning","text":"<ul> <li>Prioritize capital improvement projects using weighted decision matrices</li> <li>Evaluate alternatives across cost, risk, regulatory, and environmental impact</li> <li>Justify investment using stakeholder-aligned utility models</li> </ul>"},{"location":"UtilityTheory/#environmental-policy","title":"Environmental Policy","text":"<ul> <li>Model tradeoffs between economic development and environmental protection</li> <li>Evaluate policy impact across multiple sectors (e.g., water, air, land)</li> </ul>"},{"location":"UtilityTheory/#public-health","title":"Public Health","text":"<ul> <li>Allocate resources during emergencies based on risk-adjusted utility</li> <li>Balance equity, speed, and cost-effectiveness in public interventions</li> </ul>"},{"location":"UtilityTheory/#regulatory-decision-making","title":"Regulatory Decision-Making","text":"<ul> <li>Use structured elicitation to formalize utility functions of stakeholders</li> <li>Conduct sensitivity analysis on policy options to understand tradeoffs</li> </ul>"},{"location":"UtilityTheory/#5-elicitation-techniques","title":"5. Elicitation Techniques","text":"<p>Keeney offers several methods for eliciting utility functions and weights, such as:</p> <ul> <li>Direct rating methods: Ask stakeholders to score attributes on a 0\u20131 scale</li> <li>Lottery equivalents: Pose hypothetical gambles to assess risk tolerance</li> <li>Swing weighting: Ask how much stakeholders value a full swing from worst to best outcome in each attribute</li> </ul> <p>These techniques are designed to:</p> <ul> <li>Reduce cognitive bias</li> <li>Capture implicit preferences</li> <li>Enhance transparency and repeatability</li> </ul>"},{"location":"UtilityTheory/#6-benefits-of-keeneys-approach","title":"6. Benefits of Keeney\u2019s Approach","text":"<ul> <li>Structured clarity: Breaks down complex decisions into components</li> <li>Transparency: Makes tradeoffs and assumptions explicit</li> <li>Stakeholder alignment: Enables participatory decision-making</li> <li>Scenario testing: Supports \"what if\" analysis with utility sensitivity</li> <li>Improved innovation: Values-first thinking inspires novel solutions</li> </ul>"},{"location":"UtilityTheory/#7-related-works","title":"7. Related Works","text":"<ul> <li> <p>Keeney, R. L. &amp; Raiffa, H. (1976) Decisions with Multiple Objectives: Preferences and Value Tradeoffs   A foundational text introducing MAUT and how to construct utility functions.</p> </li> <li> <p>Keeney, R. L. (1992) Value-Focused Thinking: A Path to Creative Decisionmaking   Introduces the VFT methodology and its benefits for public policy, engineering, and business.</p> </li> </ul>"},{"location":"UtilityTheory/#8-relevance-to-water-utilities","title":"8. Relevance to Water Utilities","text":"<p>For capital planning, rate design, or program prioritization, Keeney\u2019s methods help water utilities:</p> <ul> <li>Quantify tradeoffs among cost, risk, service equity, and performance</li> <li>Justify decisions to boards, regulators, and the public</li> <li>Integrate expert judgment and public values systematically</li> <li>Enable adaptive management under climate or regulatory uncertainty</li> </ul>"},{"location":"UtilityTheory/#9-example-simplified-utility-model-for-cip-prioritization","title":"9. Example: Simplified Utility Model for CIP Prioritization","text":"Attribute Weight Utility Function Type Lifecycle Cost 0.30 Linear (lower is better) Risk Reduction 0.25 Stepwise Regulatory Impact 0.20 Threshold Community Benefit 0.15 Nonlinear (concave) Implementation Time 0.10 Linear (shorter is better) <p>Final project score: <pre><code>U(project) = 0.3\u00b7u\u2081(cost) + 0.25\u00b7u\u2082(risk) + 0.2\u00b7u\u2083(reg) + 0.15\u00b7u\u2084(benefit) + 0.1\u00b7u\u2085(time)\n</code></pre></p>"},{"location":"UtilityTheory/#10-summary","title":"10. Summary","text":"<p>Ralph Keeney\u2019s utility theory helps decision makers move beyond intuition, politics, or habit, and instead design decisions around what truly matters\u2014values. His approach has deep applicability in public sector planning, regulatory decision-making, and utility management.</p>"},{"location":"ValueEffort/","title":"Value Effort Framework for Business Objective Prioritization","text":""},{"location":"ValueEffort/#establishing-a-common-language-for-strategic-alignment","title":"Establishing a Common Language for Strategic Alignment","text":"<p>Effective prioritization starts with a shared language\u2014one rooted in business value rather than technical complexity. In many organizations, technical jargon becomes a barrier to collaboration, creating disconnects between leadership, operations, and delivery teams. To build alignment across departments and levels, prioritization must center on universally understood terms like customer impact, regulatory readiness, cost avoidance, or strategic fit.</p> <p>The Value and Effort Framework helps translate these concepts into action by evaluating initiatives through two key dimensions: business value and level of effort. Business stakeholders define value based on strategic objectives, while delivery teams assess effort based on resources, constraints, and technical complexity. This dual input balances the equation, ensuring high-impact initiatives are both desirable and realistically achievable.</p> <p>Rather than treating prioritization as a top-down directive or an internal exercise, this approach fosters a shared decision-making model. Business leaders clarify what matters most, and delivery teams provide realism on execution requirements\u2014shaping a portfolio that delivers value, earns buy-in, and builds execution momentum.</p> <p>The framework also supports transparency and recalibration over time. Prioritization isn\u2019t a one-time decision; it\u2019s a living process. As business conditions shift, stakeholder needs evolve, or new constraints emerge, the framework ensures that priorities remain visible, relevant, and actionable.</p>"},{"location":"ValueEffort/#the-value-and-effort-framework","title":"The Value and Effort Framework","text":"<p>A practical method for evaluating and sequencing initiatives, the framework bridges strategic vision and delivery constraints through four components:</p>"},{"location":"ValueEffort/#1-business-value-bv-assessment","title":"1. Business Value (BV) Assessment","text":"<p>Business stakeholders evaluate each initiative\u2019s contribution to enterprise goals. Value may relate to operational efficiency, compliance, risk mitigation, or customer experience. This ensures prioritization reflects what matters most to the organization.</p>"},{"location":"ValueEffort/#2-level-of-effort-loe-assessment","title":"2. Level of Effort (LOE) Assessment","text":"<p>Delivery teams estimate the time, complexity, and resources required. Their input helps uncover hidden execution costs and grounds plans in available capacity.</p>"},{"location":"ValueEffort/#3-value-vs-effort-matrix","title":"3. Value vs. Effort Matrix","text":"<p>Plot initiatives on a 2x2 matrix with business value on the Y-axis and effort on the X-axis. This visualization surfaces: - High-value / low-effort \u201cQuick Wins\u201d - High-effort / low-value \u201cTime Sinks\u201d - Strategic but resource-intensive \u201cStrategic Bets\u201d - Low-value / low-effort \u201cNice-to-Haves\u201d</p>"},{"location":"ValueEffort/#4-collaborative-prioritization","title":"4. Collaborative Prioritization","text":"<p>Once initiatives are mapped, each quadrant is force-ranked. Teams focus first on maximum-value, minimum-friction items while also evaluating long-term investments.</p> <p>The matrix is adaptable to quarterly planning, agile grooming, or capital budgeting\u2014supporting responsive prioritization cycles.</p>"},{"location":"ValueEffort/#t-shirt-sizing-as-a-shared-estimation-tool","title":"T-Shirt Sizing as a Shared Estimation Tool","text":"<p>To keep early effort estimates lightweight and accessible, the framework incorporates T-Shirt Sizing (S, M, L, XL). This method:</p> <ul> <li>Prevents false precision when scope is uncertain</li> <li>Encourages comparable effort discussions</li> <li>Allows faster reevaluation during reviews</li> </ul> <p>By comparing initiatives relatively, teams can quickly highlight scope or complexity differences and iterate over time.</p>"},{"location":"ValueEffort/#aligning-value-and-effort-in-a-matrix","title":"Aligning Value and Effort in a Matrix","text":"<p>The Value vs. Effort Matrix combines both dimensions into a visual format that enables teams to:</p> <ul> <li>Spot \u201cQuick Wins\u201d: High-value, low-effort tasks that build momentum</li> <li>Plan \u201cStrategic Bets\u201d: High-value, high-effort efforts worth the investment</li> <li>Question \u201cTime Sinks\u201d: High-effort, low-value projects that need decomposition</li> <li>Deprioritize \u201cNice-to-Haves\u201d: Low-effort, low-impact activities</li> </ul> <p>Each quadrant can be rank-ordered for budgeting, staffing, and tracking. Recalibration ensures continued relevance as circumstances change.</p>"},{"location":"ValueEffort/#recommendation-for-role-based-estimation","title":"Recommendation for Role-Based Estimation","text":"<p>Maintain objectivity by dividing responsibilities:</p> <ul> <li>Business stakeholders assess value based on strategy and organizational goals.</li> <li>Delivery teams assess effort based on feasibility, complexity, and constraints.</li> </ul> <p>This approach builds mutual respect, reduces assumptions, and encourages evidence-based collaboration\u2014ensuring that the process stays executable and relevant.</p>"},{"location":"ValueEffort/#summary","title":"Summary","text":"<p>The Value and Effort Framework\u2014augmented by tools like T-Shirt Sizing and visual prioritization\u2014provides more than a ranking method. It builds a shared business language for prioritization, supports strategic and technical alignment, and enables continuous recalibration over time.</p> <p>By aligning what matters with what\u2019s possible\u2014and revisiting that alignment frequently\u2014organizations can prioritize smarter, engage more effectively, and advance toward long-term goals with confidence.</p>"},{"location":"Wims/","title":"COTS Alternatives to HachWIMS, Klir, Rio, and WaterTrax","text":"<p>With Integration Support for SCADA, LIMS, GIS</p> <p>This list focuses on commercial off-the-shelf (COTS) platforms used by water utilities that support integration with existing systems such as SCADA, LIMS, GIS, and CMMS.</p>"},{"location":"Wims/#1-aquarius-aquatic-informatics-xylem","title":"1. Aquarius (Aquatic Informatics / Xylem)","text":"<ul> <li>Use Case: Time-series data management for water quality and flow.</li> <li>Integrations:</li> <li>SCADA (via APIs and secure data agents)</li> <li>LIMS (lab and field data import)</li> <li>GIS (coordinate-based mapping and overlays)</li> <li>Competes with: HachWIMS, WaterTrax</li> <li>Website: https://aquaticinformatics.com/products/aquarius/</li> </ul>"},{"location":"Wims/#2-sams-by-njbsoft","title":"2. SAMS (by NJBSoft)","text":"<ul> <li>Use Case: Permit tracking, regulatory reporting, and compliance dashboards.</li> <li>Integrations:</li> <li>SCADA (historical and real-time tags)</li> <li>LIMS (lab data ingestion)</li> <li>Oracle/SAP Financials (via ETL or API)</li> <li>Competes with: HachWIMS, Klir</li> <li>Website: https://www.njbsoft.com/</li> </ul>"},{"location":"Wims/#3-locus-eim-locus-technologies","title":"3. Locus EIM (Locus Technologies)","text":"<ul> <li>Use Case: Environmental information management with lab data focus.</li> <li>Integrations:</li> <li>LIMS (direct lab uploads and validations)</li> <li>SCADA (time-series data sync via API)</li> <li>GIS (native ESRI integration)</li> <li>Competes with: HachWIMS, WaterTrax</li> <li>Website: https://www.locustec.com/products/environmental-information-management/</li> </ul>"},{"location":"Wims/#4-swiftcomply","title":"4. SwiftComply","text":"<ul> <li>Use Case: Fats, oils, and grease (FOG), backflow, and cross-connection control.</li> <li>Integrations:</li> <li>GIS (asset and address validation)</li> <li>Customer Information Systems (via REST APIs)</li> <li>Competes with: WaterTrax, Klir</li> <li>Website: https://www.swiftcomply.com/</li> </ul>"},{"location":"Wims/#5-mapistry","title":"5. Mapistry","text":"<ul> <li>Use Case: Stormwater, air, and hazardous materials compliance.</li> <li>Integrations:</li> <li>GIS (facility maps and BMP tracking)</li> <li>Limited SCADA/LIMS unless custom developed</li> <li>Competes with: Klir, Rio</li> <li>Website: https://www.mapistry.com/</li> </ul>"},{"location":"Wims/#6-sedaru-xylem","title":"6. Sedaru (Xylem)","text":"<ul> <li>Use Case: Operational analytics, asset-driven compliance, water quality alerts.</li> <li>Integrations:</li> <li>SCADA (real-time telemetry overlays)</li> <li>GIS (deep ESRI sync)</li> <li>CMMS (Cityworks, Maximo, etc.)</li> <li>Competes with: Rio, WaterTrax</li> <li>Website: https://www.sedaru.com/</li> </ul>"},{"location":"glossary/","title":"DMBOK Glossary","text":"<p>This glossary defines core terms from the DMBOK v2 framework, tailored for use in public water utilities.</p>"},{"location":"glossary/#access-control","title":"Access Control","text":"<p>The process of defining and enforcing who can view, edit, or delete data, based on roles or permissions.</p>"},{"location":"glossary/#analytics","title":"Analytics","text":"<p>The practice of exploring, visualizing, and interpreting data to inform decisions or detect trends.</p>"},{"location":"glossary/#anomaly-detection","title":"Anomaly Detection","text":"<p>Techniques used to identify unexpected or unusual patterns in datasets \u2014 often used for monitoring or quality control.</p>"},{"location":"glossary/#application-programming-interface-api","title":"Application Programming Interface (API)","text":"<p>A defined method for systems to exchange data and services securely and consistently.</p>"},{"location":"glossary/#archiving","title":"Archiving","text":"<p>The process of moving inactive data to long-term storage while preserving integrity and retrievability.</p>"},{"location":"glossary/#as-built-data","title":"As-Built Data","text":"<p>Information that reflects the final built condition of infrastructure, often linked to GIS or engineering records.</p>"},{"location":"glossary/#audit-trail","title":"Audit Trail","text":"<p>A record of all changes to data or system access, used for traceability, security, and compliance.</p>"},{"location":"glossary/#authoritative-source","title":"Authoritative Source","text":"<p>The recognized source for a particular dataset or value, typically the system of record or a steward-approved dataset.</p>"},{"location":"glossary/#benchmarking","title":"Benchmarking","text":"<p>Comparing key performance metrics across time periods, teams, or external organizations to assess effectiveness.</p>"},{"location":"glossary/#business-glossary","title":"Business Glossary","text":"<p>A shared list of clearly defined business terms that ensures everyone uses the same language across reports, dashboards, and systems.</p>"},{"location":"glossary/#business-intelligence-bi","title":"Business Intelligence (BI)","text":"<p>Tools and processes that transform raw data into dashboards, reports, and insights for strategic or operational decisions.</p>"},{"location":"glossary/#change-management","title":"Change Management","text":"<p>The structured process of planning, approving, and documenting changes to data definitions, sources, or systems.</p>"},{"location":"glossary/#classification-scheme","title":"Classification Scheme","text":"<p>A structured system for categorizing assets, work types, or data records using consistent codes or labels.</p>"},{"location":"glossary/#compliance-reporting","title":"Compliance Reporting","text":"<p>Generating reports required by regulatory agencies (e.g., EPA, state water boards), often from the EDW or LIMS.</p>"},{"location":"glossary/#configuration-management","title":"Configuration Management","text":"<p>Tracking and managing system settings, schemas, or ETL parameters to ensure consistent environments.</p>"},{"location":"glossary/#contextual-data","title":"Contextual Data","text":"<p>Supporting information that gives meaning to primary data \u2014 such as date, location, or condition assessment.</p>"},{"location":"glossary/#controlled-vocabulary","title":"Controlled Vocabulary","text":"<p>A predefined list of acceptable terms or codes for a field, used to ensure data consistency (e.g., work order types).</p>"},{"location":"glossary/#critical-data-element","title":"Critical Data Element","text":"<p>A data field or metric considered essential to operational decisions, compliance, or reporting.</p>"},{"location":"glossary/#curation","title":"Curation","text":"<p>The ongoing process of reviewing, refining, and publishing data so it\u2019s accurate, well-documented, and ready for use.</p>"},{"location":"glossary/#dashboards","title":"Dashboards","text":"<p>Visual tools used to present KPIs or metrics using charts, gauges, and tables \u2014 often powered by BI tools.</p>"},{"location":"glossary/#data-architecture","title":"Data Architecture","text":"<p>The structural design for how data is stored, organized, and integrated across systems like CMMS, SCADA, GIS, and LIMS.</p>"},{"location":"glossary/#data-catalog","title":"Data Catalog","text":"<p>A searchable inventory of available datasets and metadata, often used to support self-service analytics and data governance.</p>"},{"location":"glossary/#data-cleansing","title":"Data Cleansing","text":"<p>The process of correcting or removing inaccurate, incomplete, or duplicated data entries.</p>"},{"location":"glossary/#data-dictionary","title":"Data Dictionary","text":"<p>A central listing of field names, types, meanings, and valid values \u2014 often used alongside metadata.</p>"},{"location":"glossary/#data-enrichment","title":"Data Enrichment","text":"<p>The process of enhancing a dataset with additional context, reference values, or derived attributes.</p>"},{"location":"glossary/#data-governance","title":"Data Governance","text":"<p>The decision-making framework that defines who is responsible for data, how decisions are made, and what policies ensure consistency and accountability.</p>"},{"location":"glossary/#data-integration","title":"Data Integration","text":"<p>The process of combining data from multiple systems to create a unified, consistent dataset, often via ETL/ELT pipelines.</p>"},{"location":"glossary/#data-lineage","title":"Data Lineage","text":"<p>The record of where data originates, how it flows through systems, and how it\u2019s transformed along the way.</p>"},{"location":"glossary/#data-literacy","title":"Data Literacy","text":"<p>The ability of staff to read, understand, and communicate using data effectively in their role.</p>"},{"location":"glossary/#data-mapping","title":"Data Mapping","text":"<p>Aligning fields from one system to another \u2014 often used when integrating or migrating datasets.</p>"},{"location":"glossary/#data-mart","title":"Data Mart","text":"<p>A focused subset of the EDW designed for a specific purpose or business domain, such as water quality or maintenance.</p>"},{"location":"glossary/#data-modeling","title":"Data Modeling","text":"<p>The process of defining entities, relationships, and business rules that structure how data is stored and used.</p>"},{"location":"glossary/#data-owner","title":"Data Owner","text":"<p>The role responsible for approving access and setting rules for how data can be used, usually aligned with business leadership.</p>"},{"location":"glossary/#data-pipeline","title":"Data Pipeline","text":"<p>A set of processes that extract, transform, and load data from source systems into a target system like an EDW.</p>"},{"location":"glossary/#data-profiling","title":"Data Profiling","text":"<p>The process of analyzing datasets to understand structure, quality, and patterns before transformation.</p>"},{"location":"glossary/#data-quality","title":"Data Quality","text":"<p>The discipline of ensuring data is accurate, complete, timely, consistent, and fit for use.</p>"},{"location":"glossary/#data-retention-policy","title":"Data Retention Policy","text":"<p>A formal guideline defining how long data is kept, and when it should be archived or deleted.</p>"},{"location":"glossary/#data-source","title":"Data Source","text":"<p>The original location or system from which a dataset originates (e.g., a SCADA historian or LIMS export).</p>"},{"location":"glossary/#data-steward","title":"Data Steward","text":"<p>The person responsible for ensuring a specific dataset is accurate, well-documented, and maintained over time.</p>"},{"location":"glossary/#data-transformation","title":"Data Transformation","text":"<p>Any process that reshapes, converts, or joins data for a downstream use case (e.g., combining tables, converting units).</p>"},{"location":"glossary/#data-use-agreement","title":"Data Use Agreement","text":"<p>A documented policy or contract defining how a dataset can be used, shared, or published.</p>"},{"location":"glossary/#data-warehouse-edw","title":"Data Warehouse (EDW)","text":"<p>A centralized system where cleaned and transformed data from operational systems is stored for analysis and reporting.</p>"},{"location":"glossary/#decoupling","title":"Decoupling","text":"<p>Designing systems so that components (like source systems and dashboards) can evolve independently.</p>"},{"location":"glossary/#document-and-content-management","title":"Document and Content Management","text":"<p>The management of unstructured information like reports, photos, and SOPs \u2014 ensuring accessibility and version control.</p>"},{"location":"glossary/#duplicate-record","title":"Duplicate Record","text":"<p>Two or more records that represent the same entity, causing redundancy and potential conflict.</p>"},{"location":"glossary/#elt-extract-load-transform","title":"ELT (Extract-Load-Transform)","text":"<p>A modern integration pattern where data is first loaded into the warehouse, then transformed using SQL or tools like dbt.</p>"},{"location":"glossary/#etl-extract-transform-load","title":"ETL (Extract-Transform-Load)","text":"<p>A traditional integration pattern where data is transformed before being loaded into the target database.</p>"},{"location":"glossary/#governance-committee","title":"Governance Committee","text":"<p>A cross-functional group that helps guide data policies, resolve conflicts, and prioritize governance work.</p>"},{"location":"glossary/#line-of-business-system","title":"Line of Business System","text":"<p>A system that supports a specific utility function, such as SCADA (operations), CMMS (maintenance), or LIMS (lab).</p>"},{"location":"glossary/#lineage","title":"Lineage","text":"<p>See 'Data Lineage.'</p>"},{"location":"glossary/#master-data","title":"Master Data","text":"<p>Key reference entities like asset types, pressure zones, or customer classes that should remain consistent across systems.</p>"},{"location":"glossary/#metadata","title":"Metadata","text":"<p>Information about data \u2014 such as its definition, origin, steward, and refresh frequency \u2014 used to support discovery and governance.</p>"},{"location":"glossary/#metrics","title":"Metrics","text":"<p>Quantifiable measures (e.g., % data completeness, number of open issues) used to track data quality, governance, and operational performance.</p>"},{"location":"glossary/#operational-data-store","title":"Operational Data Store","text":"<p>A staging area for cleaned but not yet aggregated data, often used as an intermediate step between raw source and EDW.</p>"},{"location":"glossary/#reference-data","title":"Reference Data","text":"<p>Standardized lists or codes used for validation \u2014 such as analyte names, asset categories, or work order types.</p>"},{"location":"glossary/#silver-layer","title":"Silver Layer","text":"<p>The curated, joined, and cleaned set of EDW tables ready for business use \u2014 downstream of raw source ingestion.</p>"},{"location":"glossary/#source-system","title":"Source System","text":"<p>The original system where data is generated (e.g., SCADA, CMMS), before any transformation or centralization.</p>"},{"location":"glossary/#stewardship","title":"Stewardship","text":"<p>The active process of managing and maintaining data quality, documentation, and definitions.</p>"},{"location":"glossary/#system-of-record","title":"System of Record","text":"<p>The officially designated system responsible for maintaining the authoritative version of a dataset.</p>"},{"location":"glossary/#transformation","title":"Transformation","text":"<p>The process of reshaping or standardizing data to make it consistent and useful \u2014 e.g., converting formats, units, or field names.</p>"},{"location":"glossary/#trusted-dataset","title":"Trusted Dataset","text":"<p>A dataset that meets defined governance, quality, and stewardship standards, making it suitable for enterprise use.</p>"},{"location":"glossary/#validation-rule","title":"Validation Rule","text":"<p>A logic statement or constraint used to test whether a value is complete, correct, or consistent (e.g., required fields, valid ranges).</p>"},{"location":"01_governance/","title":"Data Governance","text":"<p>Home &gt; DMBOK &gt; Data Governance</p>"},{"location":"01_governance/#data-governance","title":"Data Governance","text":"<p>Data Governance provides the foundation for treating data as a strategic, shared asset across a water utility. It defines how decisions about data are made, who is accountable for its accuracy and use, and what policies guide its management. In utilities beginning to centralize data through an Enterprise Data Warehouse, governance is often the first step toward sustainable digital practices. By identifying Data Stewards, clarifying responsibilities, and creating a shared vision, data governance aligns operations, planning, and compliance needs under a unified framework that can grow with the organization.</p>"},{"location":"01_governance/#objective","title":"Objective","text":"<p>Establish foundational data governance roles, policies, and processes to support the launch and sustainability of the utility\u2019s EDW.</p>"},{"location":"01_governance/#key-results","title":"Key Results","text":"<ul> <li>Assign Data Stewards for three initial datasets (e.g., asset registry, pump runtimes, work orders)  </li> <li>Approve a one-page data governance charter  </li> <li>Launch a quarterly Governance Committee meeting with notes and participation tracking  </li> <li>Identify executive sponsor and appoint a lead data steward  </li> </ul>"},{"location":"01_governance/#core-processes","title":"Core Processes","text":"<ul> <li>Steward nomination and onboarding  </li> <li>Data issue logging and escalation  </li> <li>Charter approval and policy versioning  </li> <li>Interdepartmental Data Ownership and decision review  </li> <li>Documentation of access policies and System of Record assignments  </li> </ul>"},{"location":"01_governance/#suggested-metrics","title":"Suggested Metrics","text":"<ul> <li>Proportion of core datasets with a named Data Steward </li> <li>Number of data issues resolved through governance workflow  </li> <li>Charter review frequency (e.g., annually)  </li> <li>Participation rate in governance meetings across departments</li> </ul>"},{"location":"02_architecture/","title":"Data Architecture","text":"<p>Home &gt; DMBOK &gt; Data Architecture</p>"},{"location":"02_architecture/#data-architecture","title":"Data Architecture","text":"<p>Data Architecture defines the blueprint for how data is structured, stored, and integrated across systems. In a water utility, this includes designing how data flows between line of business systems such as CMMS, SCADA, and GIS, and how it's centralized into an Enterprise Data Warehouse. Architecture decisions clarify the system of record for key data domains, standardize reference data, and document data movement through pipelines.</p>"},{"location":"02_architecture/#objective","title":"Objective","text":"<p>Design and document a scalable data architecture to support trustworthy, integrated data analytics.</p>"},{"location":"02_architecture/#key-results","title":"Key Results","text":"<ul> <li>Create a current-state architecture diagram (source systems, data pipelines, consumers)  </li> <li>Designate the system of record for three major domains  </li> <li>Document at least one data flow from source to warehouse  </li> <li>Publish an inventory of data sources with refresh frequencies and business owners  </li> </ul>"},{"location":"02_architecture/#core-processes","title":"Core Processes","text":"<ul> <li>Architecture diagramming and validation  </li> <li>Data domain and system of record assignment  </li> <li>Reference data harmonization  </li> <li>Metadata tagging for architectural components  </li> <li>Integration onboarding and change tracking  </li> </ul>"},{"location":"02_architecture/#suggested-metrics","title":"Suggested Metrics","text":"<ul> <li>Number of systems diagrammed with data lineage  </li> <li>Percentage of domains with confirmed system of record </li> <li>Time-to-update diagrams after system changes  </li> <li>Reduction in redundant data stores</li> </ul> <p>\u2190 Previous: Data Governance Next: Data Modeling and Design</p>"},{"location":"03_modeling/","title":"Data Modeling and Design","text":"<p>Home &gt; DMBOK &gt; Data Modeling and Design</p>"},{"location":"03_modeling/#data-modeling-and-design","title":"Data Modeling and Design","text":"<p>Data Modeling defines how data is structured, related, and represented across systems and business processes. In a water utility, this includes defining what a \u201cstation,\u201d \u201cwork order,\u201d or \u201casset\u201d means \u2014 and how those concepts are used across operational platforms. Models provide a shared language that supports consistency in analysis, data quality, and reporting.</p> <p>Early modeling efforts typically produce data dictionaries, conceptual diagrams, and help align definitions across teams. Validated models also support metadata, stewardship, and downstream reporting layers.</p>"},{"location":"03_modeling/#objective","title":"Objective","text":"<p>Define and document reusable data models that reflect key entities and support analytics across departments.</p>"},{"location":"03_modeling/#key-results","title":"Key Results","text":"<ul> <li>Draft an entity-relationship diagram for three priority domains  </li> <li>Publish a data dictionary with field types and definitions  </li> <li>Align one shared metric across multiple departments  </li> <li>Review models with both stewards and business owners  </li> </ul>"},{"location":"03_modeling/#core-processes","title":"Core Processes","text":"<ul> <li>Conceptual and logical model design  </li> <li>Definition and approval of terms and relationships  </li> <li>Metadata alignment with approved terms  </li> <li>Mapping of values across reference data </li> <li>Model version control and stewardship review  </li> </ul>"},{"location":"03_modeling/#suggested-metrics","title":"Suggested Metrics","text":"<ul> <li>Number of reviewed and published data models  </li> <li>Percentage of dashboards linked to shared models  </li> <li>Number of harmonized definitions across departments  </li> <li>Frequency of model review or change request</li> </ul> <p>\u2190 Previous: Data Architecture Next: Data Storage and Operations</p>"},{"location":"04_storage/","title":"Data Storage and Operations","text":"<p>Home &gt; DMBOK &gt; Data Storage and Operations</p>"},{"location":"04_storage/#data-storage-and-operations","title":"Data Storage and Operations","text":"<p>Data storage and operations ensure that data remains accessible, reliable, and protected over time. In a water utility, this includes storing structured data from line of business systems and reference data, ensuring archiving is in place, and managing data lifecycle practices like retention.</p> <p>For organizations adopting an Enterprise Data Warehouse,  storage operations also include tracking data pipelines, metadata, and integration refreshes.</p>"},{"location":"04_storage/#objective","title":"Objective","text":"<p>Implement resilient, policy-driven data storage to support analytics, governance, and regulatory continuity.</p>"},{"location":"04_storage/#key-results","title":"Key Results","text":"<ul> <li>Define retention policies for three high-value datasets  </li> <li>Establish automated backups and test recovery for one dataset  </li> <li>Document a storage and archiving policy reviewed by stewards  </li> <li>Classify datasets by critical data element status and storage tier  </li> </ul>"},{"location":"04_storage/#core-processes","title":"Core Processes","text":"<ul> <li>Backup creation and validation  </li> <li>Storage tier classification and archiving </li> <li>Retention policy enforcement  </li> <li>Monitoring data volume growth and access frequency  </li> <li>Periodic restore testing for continuity  </li> </ul>"},{"location":"04_storage/#suggested-metrics","title":"Suggested Metrics","text":"<ul> <li>Recovery time for most critical dataset  </li> <li>Share of datasets covered by retention policy </li> <li>Number of backup failures in last 30 days  </li> <li>Storage growth rate by system or domain</li> </ul> <p>\u2190 Previous: Data Modeling and Design Next: Data Security</p>"},{"location":"05_security/","title":"Data Security","text":"<p>Home &gt; DMBOK &gt; Data Security</p>"},{"location":"05_security/#data-security","title":"Data Security","text":"<p>Data security protects sensitive information from unauthorized access, misuse, or loss. In a water utility, this includes securing operational telemetry, reference data, and system-generated records managed in the Enterprise Data Warehouse. Strong security practices are essential to reduce risk and maintain trust as data is shared across departments and platforms.</p> <p>Security involves access control, data classification, audit trails, and data governance policies that are enforced consistently by stewards and IT administrators.</p>"},{"location":"05_security/#objective","title":"Objective","text":"<p>Establish security practices that protect data confidentiality, integrity, and availability across integrated systems and storage.</p>"},{"location":"05_security/#key-results","title":"Key Results","text":"<ul> <li>Define user roles and access control for core datasets  </li> <li>Enable secure authentication and role alignment for dashboards and EDW  </li> <li>Publish data classification levels and handling guidelines  </li> <li>Assign security stewardship responsibilities for one or more data domains  </li> </ul>"},{"location":"05_security/#core-processes","title":"Core Processes","text":"<ul> <li>Role-based access control provisioning  </li> <li>Audit logging and change tracking  </li> <li>Sensitive data classification and documentation  </li> <li>Security impact reviews within governance workflows  </li> <li>Risk mitigation and exception handling  </li> </ul>"},{"location":"05_security/#suggested-metrics","title":"Suggested Metrics","text":"<ul> <li>Percentage of datasets with documented access controls  </li> <li>Number of audit events reviewed per quarter  </li> <li>Frequency of user access reviews or role changes  </li> <li>Coverage of data classification tagging across EDW</li> </ul> <p>\u2190 Previous: Data Storage and Operations Next: Data Integration and Interoperability</p>"},{"location":"06_integration/","title":"Data Integration and Interoperability","text":"<p>Home &gt; DMBOK &gt; Data Integration and Interoperability</p>"},{"location":"06_integration/#data-integration-and-interoperability","title":"Data Integration and Interoperability","text":"<p>Data Integration connects data across systems to provide a unified, consistent view of operations and performance. In a water utility, this often means pulling data from field systems, work management tools, or lab results into an Enterprise Data Warehouse for analysis and reporting. Good integration practices ensure data is accurate, up-to-date, and traceable to its system of record.</p> <p>As more systems connect, metadata and data quality become central to understanding how values relate, while stewards and architects play a key role in defining and reviewing data pipelines.</p>"},{"location":"06_integration/#objective","title":"Objective","text":"<p>Deploy stewarded data pipelines and integration logic that connects operational data to analytics environments in a transparent and traceable way.</p>"},{"location":"06_integration/#key-results","title":"Key Results","text":"<ul> <li>Deploy and document two active integrations with refresh schedules  </li> <li>Assign stewards to monitor integration inputs and exceptions  </li> <li>Publish one flow diagram showing end-to-end movement from source to EDW  </li> <li>Resolve one field-level inconsistency using reference data</li> </ul>"},{"location":"06_integration/#core-processes","title":"Core Processes","text":"<ul> <li>Source system analysis and data mapping  </li> <li>Metadata capture for integrated datasets  </li> <li>Pipeline failure monitoring and alerting  </li> <li>Change control for schema and format shifts  </li> <li>Review of pipeline refresh timing and system impacts  </li> </ul>"},{"location":"06_integration/#suggested-metrics","title":"Suggested Metrics","text":"<ul> <li>Number of active data pipelines feeding the EDW  </li> <li>Percentage of pipelines with stewards assigned  </li> <li>Time-to-detect and resolve pipeline errors  </li> <li>Data freshness (delay from source to EDW availability)</li> </ul> <p>\u2190 Previous: Data Security Next: Document and Content Management</p>"},{"location":"07_content/","title":"Document and Content Management","text":"<p>Home &gt; DMBOK &gt; Document and Content Management</p>"},{"location":"07_content/#document-and-content-management","title":"Document and Content Management","text":"<p>Document and Content Management governs unstructured information such as reports, SOPs, maps, permits, inspection photos, and PDFs. In a water utility, these materials often complement structured data by providing the operational, regulatory, or historical context behind assets and activities.</p> <p>When properly managed, content becomes searchable, version-controlled, and linked to relevant datasets, improving transparency and supporting staff transitions. This function aligns with data governance and stewardship by assigning ownership and retention guidelines.</p>"},{"location":"07_content/#objective","title":"Objective","text":"<p>Ensure critical documents are stored, searchable, governed, and linked to supporting data systems or workflows.</p>"},{"location":"07_content/#key-results","title":"Key Results","text":"<ul> <li>Inventory 10 critical documents (e.g., permits, SOPs, internal policies)  </li> <li>Assign data stewards or content owners for high-value documents  </li> <li>Establish naming conventions and folder structure for shared drive or content system  </li> <li>Link at least one PDF or SOP to a dashboard, asset class, or glossary term  </li> </ul>"},{"location":"07_content/#core-processes","title":"Core Processes","text":"<ul> <li>Document intake, tagging, and review  </li> <li>Version control and access tracking  </li> <li>File naming and content metadata standards  </li> <li>Review and deprecation schedule enforcement  </li> <li>Integration of document references into dashboards or business glossary</li> </ul>"},{"location":"07_content/#suggested-metrics","title":"Suggested Metrics","text":"<ul> <li>Number of governed documents per domain  </li> <li>Percentage of critical SOPs updated in the past year  </li> <li>Average time to locate high-priority files during audit or response  </li> <li>Number of documents linked to analytics workflows</li> </ul> <p>\u2190 Previous: Data Integration and Interoperability Next: Reference and Master Data</p>"},{"location":"08_masterdata/","title":"Reference and Master Data","text":"<p>Home &gt; DMBOK &gt; Reference and Master Data</p>"},{"location":"08_masterdata/#reference-and-master-data","title":"Reference and Master Data","text":"<p>Reference and Master Data are shared values and entities used across multiple systems and business processes. In a water utility, this may include zone codes, facility names, pipe materials, analyte lists, and asset classifications. These data types often form the backbone of integration efforts and are critical for aligning reporting and operations.</p> <p>Reference data values are typically finite and stable (e.g., unit types, regions), while master data refers to key business entities like stations, customers, or assets with lifecycle status and system of record designations.</p>"},{"location":"08_masterdata/#objective","title":"Objective","text":"<p>Publish and maintain stewarded reference and master data values to support integration, consistency, and reporting.</p>"},{"location":"08_masterdata/#key-results","title":"Key Results","text":"<ul> <li>Identify three priority domains (e.g., assets, analytes, pressure zones) with assigned stewards  </li> <li>Publish one reference dataset to a shared environment (e.g., EDW or reporting tool)  </li> <li>Designate a system of record for each dataset  </li> <li>Document value crosswalks between two systems using the same domain  </li> </ul>"},{"location":"08_masterdata/#core-processes","title":"Core Processes","text":"<ul> <li>Stewardship assignment by domain  </li> <li>Reference value curation and approval  </li> <li>Metadata tagging for business context and refresh frequency  </li> <li>Change control for master data updates  </li> <li>Documentation of value mappings and exceptions  </li> </ul>"},{"location":"08_masterdata/#suggested-metrics","title":"Suggested Metrics","text":"<ul> <li>Number of domains with documented master data </li> <li>Percentage of dashboards using certified reference values  </li> <li>Time-to-update for code tables and dropdown lists  </li> <li>Number of system conflicts resolved via reference alignment</li> </ul> <p>\u2190 Previous: Document and Content Management Next: Data Warehousing and Business Intelligence</p>"},{"location":"09_warehousing/","title":"Data Warehousing and Business Intelligence","text":"<p>Home &gt; DMBOK &gt; Data Warehousing and Business Intelligence</p>"},{"location":"09_warehousing/#data-warehousing-and-business-intelligence","title":"Data Warehousing and Business Intelligence","text":"<p>The Enterprise Data Warehouse (EDW) serves as a central platform for reporting, analytics, and historical analysis. It brings together structured, high-quality data from multiple sources \u2014 typically operational, regulatory, and administrative \u2014 to provide a consistent foundation for decision-making.</p> <p>Business Intelligence (BI) tools allow staff to visualize, explore, and analyze this data in dashboards, charts, and reports. Both the EDW and BI layers depend heavily on reference data, data stewardship, and well-defined data models.</p>"},{"location":"09_warehousing/#objective","title":"Objective","text":"<p>Establish a trusted EDW and BI environment that enables consistent analytics, clear metric definitions, and reliable performance monitoring.</p>"},{"location":"09_warehousing/#key-results","title":"Key Results","text":"<ul> <li>Publish a data mart with conformed dimensions and summary tables  </li> <li>Launch three dashboards built from EDW data  </li> <li>Align two existing reports to reference data definitions  </li> <li>Assign stewards for high-visibility dashboards or KPIs  </li> </ul>"},{"location":"09_warehousing/#core-processes","title":"Core Processes","text":"<ul> <li>Data modeling for analytics use cases  </li> <li>Dashboard request intake and documentation  </li> <li>Metric definition and glossary alignment  </li> <li>ETL or ELT job monitoring  </li> <li>Report usage tracking and version control  </li> </ul>"},{"location":"09_warehousing/#suggested-metrics","title":"Suggested Metrics","text":"<ul> <li>Percentage of dashboards built on EDW-certified datasets  </li> <li>Number of KPIs with glossary definitions and stewards  </li> <li>Report refresh success rate  </li> <li>Average time from request to dashboard deployment</li> </ul> <p>\u2190 Previous: Reference and Master Data Next: Metadata Management</p>"},{"location":"10_metadata/","title":"Metadata Management","text":"<p>Home &gt; DMBOK &gt; Metadata Management</p>"},{"location":"10_metadata/#metadata-management","title":"Metadata Management","text":"<p>Metadata describes the structure, meaning, source, stewardship, and lifecycle of data. In a water utility, it helps teams understand what a field like \u201cruntime hours\u201d means, who owns it, and when it was last updated \u2014 providing the context behind every dataset. Strong metadata practices support data stewardship, enable data governance, and reduce onboarding time for new analysts or engineers.</p> <p>Metadata may include field descriptions, data dictionaries, business glossary terms, data pipeline refresh schedules, and source-to-target documentation.</p>"},{"location":"10_metadata/#objective","title":"Objective","text":"<p>Document and publish metadata for priority datasets to support transparency, trust, and self-service analytics.</p>"},{"location":"10_metadata/#key-results","title":"Key Results","text":"<ul> <li>Publish metadata for 10 EDW tables, including owner, refresh timing, and descriptions  </li> <li>Define 15 key business terms in a shared business glossary </li> <li>Tag 5 datasets with criticality and system of record status  </li> <li>Assign metadata stewardship responsibilities  </li> </ul>"},{"location":"10_metadata/#core-processes","title":"Core Processes","text":"<ul> <li>Metadata capture during ingestion and modeling  </li> <li>Data dictionary and glossary publishing  </li> <li>Field-level tagging with source system and steward  </li> <li>Change tracking for schema and pipeline metadata  </li> <li>Stewardship review and documentation update cycles  </li> </ul>"},{"location":"10_metadata/#suggested-metrics","title":"Suggested Metrics","text":"<ul> <li>Percentage of EDW tables with complete metadata </li> <li>Number of business glossary terms with steward and approval  </li> <li>Metadata search activity or pageviews  </li> <li>Frequency of metadata updates vs. data refresh schedule</li> </ul>"},{"location":"11_quality/","title":"Data Quality","text":"<p>Home &gt; DMBOK &gt; Data Quality</p>"},{"location":"11_quality/#data-quality","title":"Data Quality","text":"<p>Data Quality ensures that data is accurate, complete, timely, and consistent enough to support operations, reporting, and decision-making. In a water utility, poor data quality can lead to incorrect work orders, delayed compliance reports, or misinformed capital investments. A proactive quality program helps identify and fix issues early \u2014 often by combining stewardship, monitoring, and feedback loops.</p> <p>Data quality also plays a key role in adoption. When staff trust the data, they use it more consistently and effectively.</p>"},{"location":"11_quality/#objective","title":"Objective","text":"<p>Monitor and improve data quality by defining rules, assigning stewards, and resolving issues in a transparent, documented way.</p>"},{"location":"11_quality/#key-results","title":"Key Results","text":"<ul> <li>Define validation rules for three datasets (e.g., no missing runtimes, valid codes)  </li> <li>Assign data stewards to monitor and respond to flagged issues  </li> <li>Create a backlog of known issues with status and owner  </li> <li>Implement anomaly checks on at least one time-series dataset  </li> </ul>"},{"location":"11_quality/#core-processes","title":"Core Processes","text":"<ul> <li>Data profiling and rule creation  </li> <li>Issue triage, root cause analysis, and correction  </li> <li>Governance review for systemic problems  </li> <li>Feedback loops with data producers and stewards  </li> <li>Documentation and revision of validation rules </li> </ul>"},{"location":"11_quality/#suggested-metrics","title":"Suggested Metrics","text":"<ul> <li>Percentage of records passing validation rules  </li> <li>Number of unresolved issues over 30 days old  </li> <li>Time-to-resolve for high-priority errors  </li> <li>Data quality score by table, steward, or refresh</li> </ul>"}]}